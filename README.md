# NLP Using PyTorch

## Must-Read Resources

### RNN / LSTMs

* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) by [Chris Olah](https://twitter.com/ch402)
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by [Andrej Karpathy](https://twitter.com/karpathy)
* [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/) by [Edwin Chen](https://twitter.com/echen)


### Word Embedding (word2vec)

* A really good [conceptual overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) of Word2Vec from Chris McCormick 
* [First Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf) from Mikolov et al.
* [Neural Information Processing Systems, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) with improvements for Word2Vec also from Mikolov et al.


### Seq2Seq Models With Attention
* [Awesome blog](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) with amazing illustrations explaning RNN Seq2Seq models and Attention networks.


## Notebooks

### Datasets

* [Anna Karenina novel by Leo Tolstoy](https://www.dropbox.com/s/5wlnkk910ti6uo9/anna.txt?dl=0)
* [Cleaned up Wikipedia article text](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip) from [Matt Mahoney](https://twitter.com/mattmahoneyfl/)


 
