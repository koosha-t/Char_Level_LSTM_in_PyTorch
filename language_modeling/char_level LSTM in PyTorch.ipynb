{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Statistical Language Model__: A trained model to predict the next word/character given all previous words/characters.\n",
    "\n",
    "__Character-Level Language Model__: The main task of the char-level language model is to predict the next character given all previous characters in a sequence of data, i.e. generates text character by character. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on an intrigue with a French\\ngirl, who had been a governess in their family, and she had announced to\\nher husband that she could not go on living in the same house with him.\\nThis position of affairs had now lasted three days, and not only the\\nhusband and wife themselves, but all the members of their family and\\nhousehold, were painfully conscious of it. Every person in the house\\nfelt that there was no sense in their living together, and that the\\nstray people brought together by chance in any inn had more in common\\nwith one another than they, the members of the family and household of\\nthe Oblonskys. The wife did not leave her own room, the husband had not\\nbeen at home for three days. The children ran wild all over the house;\\nthe English governess quarreled with the housekeeper, and wrote to a\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding the text ## \n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:ii for ii,ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65, 22,  0,  1, 37, 76, 25, 34, 13, 57, 57, 57, 33,  0,  1,  1, 50,\n",
       "       34, 51,  0, 38, 21, 36, 21, 76, 23, 34,  0, 25, 76, 34,  0, 36, 36,\n",
       "       34,  0, 36, 21, 41, 76, 52, 34, 76, 18, 76, 25, 50, 34, 74, 15, 22,\n",
       "        0,  1,  1, 50, 34, 51,  0, 38, 21, 36, 50, 34, 21, 23, 34, 74, 15,\n",
       "       22,  0,  1,  1, 50, 34, 21, 15, 34, 21, 37, 23, 34, 42, 70, 15, 57,\n",
       "       70,  0, 50, 43, 57, 57, 17, 18, 76, 25, 50, 37, 22, 21, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    #n_labels = max(arr.flatten()) + 1\n",
    "    \n",
    "    one_hot = np.zeros(shape = (np.multiply(*arr.shape) , n_labels))\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = np.array([[1,2,3,7],[5,3,2,8]])\n",
    "one_hot = one_hot_encode(test_seq,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N: batch size\n",
    "# M: sequence length\n",
    "# K: total number of batches\n",
    "\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    \n",
    "    # Number of matches we can make from the input array\n",
    "    n_batches = len(arr) // (batch_size * seq_length)\n",
    "    \n",
    "    # keeping enoough character to make full batches\n",
    "    arr = arr[:n_batches * batch_size * seq_length]\n",
    "    \n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterating over the batches\n",
    "    for n in range(0, arr.shape[1] , seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1], y[:,-1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1], y[:,-1] = x[:, 1:], arr[:,0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the get_batch function\n",
    "\n",
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[65, 22,  0,  1, 37, 76, 25, 34, 13, 57, 57, 57, 33,  0,  1,  1,\n",
       "         50, 34, 51,  0, 38, 21, 36, 21, 76, 23, 34,  0, 25, 76, 34,  0,\n",
       "         36, 36, 34,  0, 36, 21, 41, 76, 52, 34, 76, 18, 76, 25, 50, 34,\n",
       "         74, 15],\n",
       "        [23, 42, 15, 34, 37, 22,  0, 37, 34,  0, 37, 37, 25,  0, 63, 37,\n",
       "         76, 81, 34, 22, 76, 25, 34,  0, 37, 37, 76, 15, 37, 21, 42, 15,\n",
       "         34, 70,  0, 23, 34, 22, 76, 25, 34, 22, 74, 23, 24,  0, 15, 81,\n",
       "         43, 34],\n",
       "        [76, 15, 81, 34, 42, 25, 34,  0, 34, 51, 42, 76, 56, 34, 22, 76,\n",
       "         34,  0, 18, 42, 21, 81, 76, 81, 34, 22, 21, 23, 34, 51,  0, 37,\n",
       "         22, 76, 25, 43, 34, 33, 76, 57, 36, 42, 42, 41, 76, 81, 34, 25,\n",
       "         42, 74],\n",
       "        [23, 34, 37, 22, 76, 34, 63, 22, 21, 76, 51, 34, 37, 22, 42, 74,\n",
       "         71, 22, 34, 22, 21, 81, 81, 76, 15, 57, 21, 15, 37, 76, 25, 76,\n",
       "         23, 37, 34, 42, 51, 34, 22, 21, 23, 34, 36, 21, 51, 76, 56, 34,\n",
       "         42, 51],\n",
       "        [34, 23,  0, 70, 34, 22, 76, 25, 34, 37, 76,  0, 25, 35, 23, 37,\n",
       "          0, 21, 15, 76, 81, 56, 34,  1, 21, 37, 21, 51, 74, 36, 56, 34,\n",
       "         23, 70, 76, 76, 37, 34, 51,  0, 63, 76, 56, 57, 38, 21, 23, 76,\n",
       "         25,  0],\n",
       "        [63, 74, 23, 23, 21, 42, 15, 34,  0, 15, 81, 34,  0, 15,  0, 36,\n",
       "         50, 23, 21, 23, 56, 34, 70,  0, 23, 34, 21, 15, 34,  1, 25, 21,\n",
       "         15, 63, 21,  1, 36, 76, 34, 81, 21, 23,  0, 71, 25, 76, 76,  0,\n",
       "         24, 36],\n",
       "        [34, 12, 15, 15,  0, 34, 22,  0, 81, 34, 23,  0, 21, 81, 34, 37,\n",
       "         22,  0, 37, 34, 78, 42, 36, 36, 50, 34, 70, 42, 74, 36, 81, 34,\n",
       "         76, 11, 63, 74, 23, 76, 34, 21, 37, 43, 34, 12, 15, 81, 34, 37,\n",
       "         22, 21],\n",
       "        [ 8, 24, 36, 42, 15, 23, 41, 50, 43, 34, 66,  6, 74, 37, 34, 55,\n",
       "         37, 22, 76, 50, 55, 34, 63,  0, 15, 15, 42, 37, 34, 71, 25,  0,\n",
       "         23,  1, 34, 37, 22,  0, 37, 56, 57, 55, 37, 22, 76, 50, 55, 34,\n",
       "          0, 25]]),\n",
       " array([[22,  0,  1, 37, 76, 25, 34, 13, 57, 57, 57, 33,  0,  1,  1, 50,\n",
       "         34, 51,  0, 38, 21, 36, 21, 76, 23, 34,  0, 25, 76, 34,  0, 36,\n",
       "         36, 34,  0, 36, 21, 41, 76, 52, 34, 76, 18, 76, 25, 50, 34, 74,\n",
       "         15, 22],\n",
       "        [42, 15, 34, 37, 22,  0, 37, 34,  0, 37, 37, 25,  0, 63, 37, 76,\n",
       "         81, 34, 22, 76, 25, 34,  0, 37, 37, 76, 15, 37, 21, 42, 15, 34,\n",
       "         70,  0, 23, 34, 22, 76, 25, 34, 22, 74, 23, 24,  0, 15, 81, 43,\n",
       "         34, 66],\n",
       "        [15, 81, 34, 42, 25, 34,  0, 34, 51, 42, 76, 56, 34, 22, 76, 34,\n",
       "          0, 18, 42, 21, 81, 76, 81, 34, 22, 21, 23, 34, 51,  0, 37, 22,\n",
       "         76, 25, 43, 34, 33, 76, 57, 36, 42, 42, 41, 76, 81, 34, 25, 42,\n",
       "         74, 15],\n",
       "        [34, 37, 22, 76, 34, 63, 22, 21, 76, 51, 34, 37, 22, 42, 74, 71,\n",
       "         22, 34, 22, 21, 81, 81, 76, 15, 57, 21, 15, 37, 76, 25, 76, 23,\n",
       "         37, 34, 42, 51, 34, 22, 21, 23, 34, 36, 21, 51, 76, 56, 34, 42,\n",
       "         51, 34],\n",
       "        [23,  0, 70, 34, 22, 76, 25, 34, 37, 76,  0, 25, 35, 23, 37,  0,\n",
       "         21, 15, 76, 81, 56, 34,  1, 21, 37, 21, 51, 74, 36, 56, 34, 23,\n",
       "         70, 76, 76, 37, 34, 51,  0, 63, 76, 56, 57, 38, 21, 23, 76, 25,\n",
       "          0, 24],\n",
       "        [74, 23, 23, 21, 42, 15, 34,  0, 15, 81, 34,  0, 15,  0, 36, 50,\n",
       "         23, 21, 23, 56, 34, 70,  0, 23, 34, 21, 15, 34,  1, 25, 21, 15,\n",
       "         63, 21,  1, 36, 76, 34, 81, 21, 23,  0, 71, 25, 76, 76,  0, 24,\n",
       "         36, 76],\n",
       "        [12, 15, 15,  0, 34, 22,  0, 81, 34, 23,  0, 21, 81, 34, 37, 22,\n",
       "          0, 37, 34, 78, 42, 36, 36, 50, 34, 70, 42, 74, 36, 81, 34, 76,\n",
       "         11, 63, 74, 23, 76, 34, 21, 37, 43, 34, 12, 15, 81, 34, 37, 22,\n",
       "         21, 23],\n",
       "        [24, 36, 42, 15, 23, 41, 50, 43, 34, 66,  6, 74, 37, 34, 55, 37,\n",
       "         22, 76, 50, 55, 34, 63,  0, 15, 15, 42, 37, 34, 71, 25,  0, 23,\n",
       "          1, 34, 37, 22,  0, 37, 56, 57, 55, 37, 22, 76, 50, 55, 34,  0,\n",
       "         25, 76]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, chars, n_hidden = 256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.chars = chars\n",
    "        \n",
    "        self.int2chars = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch : ii for ii,ch in self.int2chars.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = len(self.chars), hidden_size = n_hidden,  num_layers = n_layers, \\\n",
    "                             dropout = self.drop_prob, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(self.n_hidden, len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # creating two new tensors with size n_layers * batch_size * n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of the LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "            \n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, \\\n",
    "          val_frac=0.1, print_every=10, epoch_offset = 0,save_path = \"model.pt\"):\n",
    "    '''\n",
    "        Arguments:\n",
    "            net: CharRNN network\n",
    "            data: text data to train the network\n",
    "            epochs: number of epochs\n",
    "            batch_size: number of mini-sequences per mini-batch\n",
    "            seq_length: Number of character steps per mini-batch\n",
    "            lr: learning rate\n",
    "            clip: gradient clipping\n",
    "            val_frac: Fraction of data to hold out for validation\n",
    "            print_every: number of steps for printing training and validation\n",
    "    '''\n",
    "    \n",
    "    # Making all parameters of the model Double\n",
    "    net.double()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # creating training and validation data\n",
    "    val_idx = int(len(data) * (1 - val_frac))\n",
    "    data , val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epoch_offset,  epoch_offset + epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                inputs , targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            #creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses=[]\n",
    "                net.eval()\n",
    "                for x,y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x,y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x,y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                    \n",
    "                    val_losses.append(val_loss.item())\n",
    "            \n",
    "                net.train()\n",
    "                \n",
    "                new_val_loss = np.mean(val_losses)\n",
    "                \n",
    "                if new_val_loss < valid_loss_min:\n",
    "                    valid_loss_min = new_val_loss\n",
    "                    torch.save(net.state_dict(), save_path)\n",
    "                \n",
    "                print(\"Epoch: {}...\".format(e+1),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.4f}\".format(new_val_loss))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2934... Val Loss: 3.2490\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1470... Val Loss: 3.1393\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1443... Val Loss: 3.1257\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1156... Val Loss: 3.1191\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1471... Val Loss: 3.1178\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1196... Val Loss: 3.1162\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1104... Val Loss: 3.1154\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1254... Val Loss: 3.1130\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1240... Val Loss: 3.1071\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1062... Val Loss: 3.0989\n",
      "Epoch: 1/20... Step: 110... Loss: 3.0865... Val Loss: 3.0667\n",
      "Epoch: 1/20... Step: 120... Loss: 2.9947... Val Loss: 2.9856\n",
      "Epoch: 1/20... Step: 130... Loss: 2.9743... Val Loss: 2.9341\n",
      "Epoch: 2/20... Step: 140... Loss: 2.8703... Val Loss: 2.8226\n",
      "Epoch: 2/20... Step: 150... Loss: 2.7537... Val Loss: 2.7147\n",
      "Epoch: 2/20... Step: 160... Loss: 2.6497... Val Loss: 2.6166\n",
      "Epoch: 2/20... Step: 170... Loss: 2.5541... Val Loss: 2.5426\n",
      "Epoch: 2/20... Step: 180... Loss: 2.5209... Val Loss: 2.4935\n",
      "Epoch: 2/20... Step: 190... Loss: 2.4676... Val Loss: 2.4609\n",
      "Epoch: 2/20... Step: 200... Loss: 2.4552... Val Loss: 2.4194\n",
      "Epoch: 2/20... Step: 210... Loss: 2.4140... Val Loss: 2.3955\n",
      "Epoch: 2/20... Step: 220... Loss: 2.3850... Val Loss: 2.3620\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3672... Val Loss: 2.3350\n",
      "Epoch: 2/20... Step: 240... Loss: 2.3486... Val Loss: 2.3141\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2884... Val Loss: 2.2849\n",
      "Epoch: 2/20... Step: 260... Loss: 2.2648... Val Loss: 2.2587\n",
      "Epoch: 2/20... Step: 270... Loss: 2.2677... Val Loss: 2.2335\n",
      "Epoch: 3/20... Step: 280... Loss: 2.2587... Val Loss: 2.2103\n",
      "Epoch: 3/20... Step: 290... Loss: 2.2183... Val Loss: 2.1876\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1998... Val Loss: 2.1635\n",
      "Epoch: 3/20... Step: 310... Loss: 2.1667... Val Loss: 2.1413\n",
      "Epoch: 3/20... Step: 320... Loss: 2.1353... Val Loss: 2.1201\n",
      "Epoch: 3/20... Step: 330... Loss: 2.1032... Val Loss: 2.1029\n",
      "Epoch: 3/20... Step: 340... Loss: 2.1167... Val Loss: 2.0848\n",
      "Epoch: 3/20... Step: 350... Loss: 2.1153... Val Loss: 2.0654\n",
      "Epoch: 3/20... Step: 360... Loss: 2.0413... Val Loss: 2.0480\n",
      "Epoch: 3/20... Step: 370... Loss: 2.0652... Val Loss: 2.0272\n",
      "Epoch: 3/20... Step: 380... Loss: 2.0443... Val Loss: 2.0133\n",
      "Epoch: 3/20... Step: 390... Loss: 2.0161... Val Loss: 2.0000\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9858... Val Loss: 1.9793\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9929... Val Loss: 1.9603\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9857... Val Loss: 1.9498\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9715... Val Loss: 1.9327\n",
      "Epoch: 4/20... Step: 440... Loss: 1.9528... Val Loss: 1.9198\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8903... Val Loss: 1.9051\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8852... Val Loss: 1.8909\n",
      "Epoch: 4/20... Step: 470... Loss: 1.9090... Val Loss: 1.8811\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8980... Val Loss: 1.8699\n",
      "Epoch: 4/20... Step: 490... Loss: 1.9013... Val Loss: 1.8587\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8884... Val Loss: 1.8439\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8660... Val Loss: 1.8329\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8814... Val Loss: 1.8228\n",
      "Epoch: 4/20... Step: 530... Loss: 1.8399... Val Loss: 1.8128\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7977... Val Loss: 1.8025\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8384... Val Loss: 1.7902\n",
      "Epoch: 5/20... Step: 560... Loss: 1.8127... Val Loss: 1.7816\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7850... Val Loss: 1.7717\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7738... Val Loss: 1.7613\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7744... Val Loss: 1.7506\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7631... Val Loss: 1.7439\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7513... Val Loss: 1.7376\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7413... Val Loss: 1.7290\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7685... Val Loss: 1.7190\n",
      "Epoch: 5/20... Step: 640... Loss: 1.7354... Val Loss: 1.7117\n",
      "Epoch: 5/20... Step: 650... Loss: 1.7299... Val Loss: 1.7054\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6922... Val Loss: 1.6974\n",
      "Epoch: 5/20... Step: 670... Loss: 1.7245... Val Loss: 1.6884\n",
      "Epoch: 5/20... Step: 680... Loss: 1.7140... Val Loss: 1.6830\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6974... Val Loss: 1.6766\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6850... Val Loss: 1.6687\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6864... Val Loss: 1.6643\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6762... Val Loss: 1.6559\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6885... Val Loss: 1.6508\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6464... Val Loss: 1.6440\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6319... Val Loss: 1.6386\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6702... Val Loss: 1.6354\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6437... Val Loss: 1.6304\n",
      "Epoch: 6/20... Step: 780... Loss: 1.6342... Val Loss: 1.6222\n",
      "Epoch: 6/20... Step: 790... Loss: 1.6196... Val Loss: 1.6187\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6271... Val Loss: 1.6128\n",
      "Epoch: 6/20... Step: 810... Loss: 1.6109... Val Loss: 1.6084\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5876... Val Loss: 1.6060\n",
      "Epoch: 6/20... Step: 830... Loss: 1.6327... Val Loss: 1.5964\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5800... Val Loss: 1.5913\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5918... Val Loss: 1.5905\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5844... Val Loss: 1.5846\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5947... Val Loss: 1.5790\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5859... Val Loss: 1.5766\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5899... Val Loss: 1.5729\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5744... Val Loss: 1.5679\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5401... Val Loss: 1.5606\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5758... Val Loss: 1.5592\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5459... Val Loss: 1.5560\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5549... Val Loss: 1.5531\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5606... Val Loss: 1.5480\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5654... Val Loss: 1.5421\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5631... Val Loss: 1.5394\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5451... Val Loss: 1.5359\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5431... Val Loss: 1.5318\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5288... Val Loss: 1.5267\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5686... Val Loss: 1.5237\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5383... Val Loss: 1.5200\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.5250... Val Loss: 1.5192\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5196... Val Loss: 1.5190\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.5029... Val Loss: 1.5113\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.5046... Val Loss: 1.5076\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.5159... Val Loss: 1.5075\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.5157... Val Loss: 1.5041\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4917... Val Loss: 1.5011\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4871... Val Loss: 1.4959\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4989... Val Loss: 1.4938\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.5111... Val Loss: 1.4931\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4940... Val Loss: 1.4873\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4956... Val Loss: 1.4853\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.5119... Val Loss: 1.4845\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4670... Val Loss: 1.4796\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4795... Val Loss: 1.4787\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4746... Val Loss: 1.4798\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.5052... Val Loss: 1.4756\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4525... Val Loss: 1.4733\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4662... Val Loss: 1.4698\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4640... Val Loss: 1.4668\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4495... Val Loss: 1.4663\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4517... Val Loss: 1.4603\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4585... Val Loss: 1.4591\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4575... Val Loss: 1.4608\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4527... Val Loss: 1.4527\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4627... Val Loss: 1.4509\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4542... Val Loss: 1.4503\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4386... Val Loss: 1.4466\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4651... Val Loss: 1.4466\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4254... Val Loss: 1.4453\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4306... Val Loss: 1.4452\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.4132... Val Loss: 1.4398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.4076... Val Loss: 1.4387\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.4167... Val Loss: 1.4364\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3937... Val Loss: 1.4366\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4342... Val Loss: 1.4304\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4441... Val Loss: 1.4278\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4534... Val Loss: 1.4299\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4605... Val Loss: 1.4258\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4489... Val Loss: 1.4208\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4122... Val Loss: 1.4230\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4362... Val Loss: 1.4172\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3694... Val Loss: 1.4203\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3932... Val Loss: 1.4161\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3848... Val Loss: 1.4191\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.4043... Val Loss: 1.4135\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.4001... Val Loss: 1.4142\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3797... Val Loss: 1.4138\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3718... Val Loss: 1.4109\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.4026... Val Loss: 1.4050\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4506... Val Loss: 1.4050\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.4079... Val Loss: 1.4050\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.4092... Val Loss: 1.4044\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4167... Val Loss: 1.4010\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3688... Val Loss: 1.4010\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3512... Val Loss: 1.3982\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3474... Val Loss: 1.3980\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3729... Val Loss: 1.3997\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3593... Val Loss: 1.4020\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3647... Val Loss: 1.3953\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3850... Val Loss: 1.3973\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3582... Val Loss: 1.3948\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3288... Val Loss: 1.3908\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3959... Val Loss: 1.3881\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3691... Val Loss: 1.3873\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3735... Val Loss: 1.3857\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3474... Val Loss: 1.3841\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3546... Val Loss: 1.3825\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3370... Val Loss: 1.3811\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3418... Val Loss: 1.3788\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3791... Val Loss: 1.3778\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3405... Val Loss: 1.3782\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3124... Val Loss: 1.3841\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3428... Val Loss: 1.3779\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3634... Val Loss: 1.3779\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3422... Val Loss: 1.3743\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3245... Val Loss: 1.3763\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3478... Val Loss: 1.3714\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3633... Val Loss: 1.3706\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3363... Val Loss: 1.3693\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3555... Val Loss: 1.3679\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2993... Val Loss: 1.3687\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2803... Val Loss: 1.3651\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3486... Val Loss: 1.3656\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3445... Val Loss: 1.3635\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3350... Val Loss: 1.3643\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3651... Val Loss: 1.3634\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3404... Val Loss: 1.3619\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3288... Val Loss: 1.3637\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3286... Val Loss: 1.3608\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2934... Val Loss: 1.3597\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3406... Val Loss: 1.3574\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3151... Val Loss: 1.3602\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3155... Val Loss: 1.3534\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3139... Val Loss: 1.3534\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.3006... Val Loss: 1.3549\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.3078... Val Loss: 1.3502\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2922... Val Loss: 1.3493\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.3206... Val Loss: 1.3467\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3287... Val Loss: 1.3484\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2944... Val Loss: 1.3498\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.3075... Val Loss: 1.3475\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.3023... Val Loss: 1.3481\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.3086... Val Loss: 1.3481\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3192... Val Loss: 1.3461\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.3044... Val Loss: 1.3431\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.3154... Val Loss: 1.3437\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2973... Val Loss: 1.3453\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2892... Val Loss: 1.3405\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.3042... Val Loss: 1.3413\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2770... Val Loss: 1.3410\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2953... Val Loss: 1.3383\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.3189... Val Loss: 1.3368\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2880... Val Loss: 1.3381\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2886... Val Loss: 1.3363\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2768... Val Loss: 1.3365\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.3110... Val Loss: 1.3374\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2825... Val Loss: 1.3350\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2445... Val Loss: 1.3331\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2968... Val Loss: 1.3325\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2741... Val Loss: 1.3291\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2791... Val Loss: 1.3294\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2661... Val Loss: 1.3322\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2732... Val Loss: 1.3280\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2845... Val Loss: 1.3284\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2783... Val Loss: 1.3248\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2729... Val Loss: 1.3255\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2462... Val Loss: 1.3256\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2758... Val Loss: 1.3255\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2664... Val Loss: 1.3236\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2693... Val Loss: 1.3240\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2852... Val Loss: 1.3251\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2840... Val Loss: 1.3213\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2901... Val Loss: 1.3194\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2574... Val Loss: 1.3210\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2642... Val Loss: 1.3209\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2601... Val Loss: 1.3185\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2873... Val Loss: 1.3172\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2827... Val Loss: 1.3194\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2690... Val Loss: 1.3158\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2744... Val Loss: 1.3160\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2643... Val Loss: 1.3212\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2439... Val Loss: 1.3167\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2649... Val Loss: 1.3171\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2661... Val Loss: 1.3145\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2532... Val Loss: 1.3154\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2495... Val Loss: 1.3162\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2526... Val Loss: 1.3139\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2621... Val Loss: 1.3171\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2725... Val Loss: 1.3111\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2780... Val Loss: 1.3110\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2826... Val Loss: 1.3102\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2477... Val Loss: 1.3153\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2631... Val Loss: 1.3103\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2441... Val Loss: 1.3108\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2735... Val Loss: 1.3120\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2373... Val Loss: 1.3096\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2386... Val Loss: 1.3099\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2466... Val Loss: 1.3089\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2427... Val Loss: 1.3095\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2398... Val Loss: 1.3077\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2580... Val Loss: 1.3059\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2488... Val Loss: 1.3123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.2602... Val Loss: 1.3032\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2596... Val Loss: 1.3002\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2579... Val Loss: 1.2985\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2449... Val Loss: 1.3035\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2532... Val Loss: 1.2979\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2266... Val Loss: 1.2982\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2219... Val Loss: 1.3026\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2182... Val Loss: 1.3017\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.2190... Val Loss: 1.3026\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2259... Val Loss: 1.3020\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2165... Val Loss: 1.3010\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2574... Val Loss: 1.2999\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2761... Val Loss: 1.2996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size= batch_size, \\\n",
    "      seq_length= seq_length, lr=0.001, print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),\"char_level_lstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"char_level_lstm.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21... Step: 10... Loss: 1.2595... Val Loss: 1.2996\n",
      "Epoch: 21... Step: 20... Loss: 1.2836... Val Loss: 1.3003\n",
      "Epoch: 21... Step: 30... Loss: 1.2748... Val Loss: 1.2965\n",
      "Epoch: 21... Step: 40... Loss: 1.2438... Val Loss: 1.2938\n",
      "Epoch: 21... Step: 50... Loss: 1.2566... Val Loss: 1.2989\n",
      "Epoch: 21... Step: 60... Loss: 1.1864... Val Loss: 1.2957\n",
      "Epoch: 21... Step: 70... Loss: 1.2219... Val Loss: 1.2964\n",
      "Epoch: 21... Step: 80... Loss: 1.2092... Val Loss: 1.2990\n",
      "Epoch: 21... Step: 90... Loss: 1.2404... Val Loss: 1.2961\n",
      "Epoch: 21... Step: 100... Loss: 1.2278... Val Loss: 1.2972\n",
      "Epoch: 21... Step: 110... Loss: 1.2098... Val Loss: 1.2976\n",
      "Epoch: 21... Step: 120... Loss: 1.1938... Val Loss: 1.2982\n",
      "Epoch: 21... Step: 130... Loss: 1.2347... Val Loss: 1.2939\n",
      "Epoch: 22... Step: 140... Loss: 1.2773... Val Loss: 1.2944\n",
      "Epoch: 22... Step: 150... Loss: 1.2365... Val Loss: 1.2990\n",
      "Epoch: 22... Step: 160... Loss: 1.2368... Val Loss: 1.2908\n",
      "Epoch: 22... Step: 170... Loss: 1.2598... Val Loss: 1.2930\n",
      "Epoch: 22... Step: 180... Loss: 1.2118... Val Loss: 1.2927\n",
      "Epoch: 22... Step: 190... Loss: 1.1946... Val Loss: 1.2975\n",
      "Epoch: 22... Step: 200... Loss: 1.1902... Val Loss: 1.2944\n",
      "Epoch: 22... Step: 210... Loss: 1.2084... Val Loss: 1.2915\n",
      "Epoch: 22... Step: 220... Loss: 1.2092... Val Loss: 1.2925\n",
      "Epoch: 22... Step: 230... Loss: 1.2044... Val Loss: 1.2910\n",
      "Epoch: 22... Step: 240... Loss: 1.2235... Val Loss: 1.2923\n",
      "Epoch: 22... Step: 250... Loss: 1.2099... Val Loss: 1.2904\n",
      "Epoch: 22... Step: 260... Loss: 1.1815... Val Loss: 1.2906\n",
      "Epoch: 22... Step: 270... Loss: 1.2480... Val Loss: 1.2867\n",
      "Epoch: 23... Step: 280... Loss: 1.2087... Val Loss: 1.2907\n",
      "Epoch: 23... Step: 290... Loss: 1.2218... Val Loss: 1.2926\n",
      "Epoch: 23... Step: 300... Loss: 1.2043... Val Loss: 1.2873\n",
      "Epoch: 23... Step: 310... Loss: 1.2020... Val Loss: 1.2825\n",
      "Epoch: 23... Step: 320... Loss: 1.1920... Val Loss: 1.2858\n",
      "Epoch: 23... Step: 330... Loss: 1.1920... Val Loss: 1.2882\n",
      "Epoch: 23... Step: 340... Loss: 1.2297... Val Loss: 1.2836\n",
      "Epoch: 23... Step: 350... Loss: 1.1956... Val Loss: 1.2839\n",
      "Epoch: 23... Step: 360... Loss: 1.1735... Val Loss: 1.2870\n",
      "Epoch: 23... Step: 370... Loss: 1.1957... Val Loss: 1.2831\n",
      "Epoch: 23... Step: 380... Loss: 1.2213... Val Loss: 1.2818\n",
      "Epoch: 23... Step: 390... Loss: 1.1972... Val Loss: 1.2814\n",
      "Epoch: 23... Step: 400... Loss: 1.1838... Val Loss: 1.2812\n",
      "Epoch: 23... Step: 410... Loss: 1.2067... Val Loss: 1.2789\n",
      "Epoch: 24... Step: 420... Loss: 1.2189... Val Loss: 1.2793\n",
      "Epoch: 24... Step: 430... Loss: 1.1890... Val Loss: 1.2830\n",
      "Epoch: 24... Step: 440... Loss: 1.2117... Val Loss: 1.2801\n",
      "Epoch: 24... Step: 450... Loss: 1.1699... Val Loss: 1.2806\n",
      "Epoch: 24... Step: 460... Loss: 1.1439... Val Loss: 1.2817\n",
      "Epoch: 24... Step: 470... Loss: 1.2108... Val Loss: 1.2769\n",
      "Epoch: 24... Step: 480... Loss: 1.2063... Val Loss: 1.2779\n",
      "Epoch: 24... Step: 490... Loss: 1.2015... Val Loss: 1.2752\n",
      "Epoch: 24... Step: 500... Loss: 1.2326... Val Loss: 1.2802\n",
      "Epoch: 24... Step: 510... Loss: 1.1989... Val Loss: 1.2802\n",
      "Epoch: 24... Step: 520... Loss: 1.1987... Val Loss: 1.2760\n",
      "Epoch: 24... Step: 530... Loss: 1.2005... Val Loss: 1.2718\n",
      "Epoch: 24... Step: 540... Loss: 1.1660... Val Loss: 1.2705\n",
      "Epoch: 24... Step: 550... Loss: 1.2142... Val Loss: 1.2672\n",
      "Epoch: 25... Step: 560... Loss: 1.1894... Val Loss: 1.2820\n",
      "Epoch: 25... Step: 570... Loss: 1.1883... Val Loss: 1.2779\n",
      "Epoch: 25... Step: 580... Loss: 1.1740... Val Loss: 1.2693\n",
      "Epoch: 25... Step: 590... Loss: 1.1718... Val Loss: 1.2703\n",
      "Epoch: 25... Step: 600... Loss: 1.1680... Val Loss: 1.2741\n",
      "Epoch: 25... Step: 610... Loss: 1.1630... Val Loss: 1.2711\n",
      "Epoch: 25... Step: 620... Loss: 1.1886... Val Loss: 1.2668\n",
      "Epoch: 25... Step: 630... Loss: 1.1945... Val Loss: 1.2686\n",
      "Epoch: 25... Step: 640... Loss: 1.1612... Val Loss: 1.2744\n",
      "Epoch: 25... Step: 650... Loss: 1.1871... Val Loss: 1.2770\n",
      "Epoch: 25... Step: 660... Loss: 1.1652... Val Loss: 1.2688\n",
      "Epoch: 25... Step: 670... Loss: 1.1817... Val Loss: 1.2663\n",
      "Epoch: 25... Step: 680... Loss: 1.1853... Val Loss: 1.2664\n",
      "Epoch: 25... Step: 690... Loss: 1.1855... Val Loss: 1.2677\n",
      "Epoch: 26... Step: 700... Loss: 1.1879... Val Loss: 1.2739\n",
      "Epoch: 26... Step: 710... Loss: 1.1801... Val Loss: 1.2688\n",
      "Epoch: 26... Step: 720... Loss: 1.1613... Val Loss: 1.2641\n",
      "Epoch: 26... Step: 730... Loss: 1.1824... Val Loss: 1.2694\n",
      "Epoch: 26... Step: 740... Loss: 1.1576... Val Loss: 1.2744\n",
      "Epoch: 26... Step: 750... Loss: 1.1770... Val Loss: 1.2673\n",
      "Epoch: 26... Step: 760... Loss: 1.2081... Val Loss: 1.2683\n",
      "Epoch: 26... Step: 770... Loss: 1.1709... Val Loss: 1.2627\n",
      "Epoch: 26... Step: 780... Loss: 1.1687... Val Loss: 1.2645\n",
      "Epoch: 26... Step: 790... Loss: 1.1630... Val Loss: 1.2665\n",
      "Epoch: 26... Step: 800... Loss: 1.1829... Val Loss: 1.2586\n",
      "Epoch: 26... Step: 810... Loss: 1.1687... Val Loss: 1.2614\n",
      "Epoch: 26... Step: 820... Loss: 1.1261... Val Loss: 1.2581\n",
      "Epoch: 26... Step: 830... Loss: 1.1804... Val Loss: 1.2610\n",
      "Epoch: 27... Step: 840... Loss: 1.1590... Val Loss: 1.2669\n",
      "Epoch: 27... Step: 850... Loss: 1.1696... Val Loss: 1.2619\n",
      "Epoch: 27... Step: 860... Loss: 1.1476... Val Loss: 1.2577\n",
      "Epoch: 27... Step: 870... Loss: 1.1657... Val Loss: 1.2637\n",
      "Epoch: 27... Step: 880... Loss: 1.1671... Val Loss: 1.2634\n",
      "Epoch: 27... Step: 890... Loss: 1.1795... Val Loss: 1.2646\n",
      "Epoch: 27... Step: 900... Loss: 1.1796... Val Loss: 1.2615\n",
      "Epoch: 27... Step: 910... Loss: 1.1378... Val Loss: 1.2569\n",
      "Epoch: 27... Step: 920... Loss: 1.1616... Val Loss: 1.2617\n",
      "Epoch: 27... Step: 930... Loss: 1.1507... Val Loss: 1.2639\n",
      "Epoch: 27... Step: 940... Loss: 1.1518... Val Loss: 1.2587\n",
      "Epoch: 27... Step: 950... Loss: 1.1658... Val Loss: 1.2615\n",
      "Epoch: 27... Step: 960... Loss: 1.1660... Val Loss: 1.2599\n",
      "Epoch: 27... Step: 970... Loss: 1.1813... Val Loss: 1.2580\n",
      "Epoch: 28... Step: 980... Loss: 1.1597... Val Loss: 1.2698\n",
      "Epoch: 28... Step: 990... Loss: 1.1578... Val Loss: 1.2603\n",
      "Epoch: 28... Step: 1000... Loss: 1.1462... Val Loss: 1.2573\n",
      "Epoch: 28... Step: 1010... Loss: 1.1725... Val Loss: 1.2658\n",
      "Epoch: 28... Step: 1020... Loss: 1.1733... Val Loss: 1.2636\n",
      "Epoch: 28... Step: 1030... Loss: 1.1601... Val Loss: 1.2595\n",
      "Epoch: 28... Step: 1040... Loss: 1.1635... Val Loss: 1.2567\n",
      "Epoch: 28... Step: 1050... Loss: 1.1472... Val Loss: 1.2560\n",
      "Epoch: 28... Step: 1060... Loss: 1.1401... Val Loss: 1.2577\n",
      "Epoch: 28... Step: 1070... Loss: 1.1668... Val Loss: 1.2577\n",
      "Epoch: 28... Step: 1080... Loss: 1.1519... Val Loss: 1.2563\n",
      "Epoch: 28... Step: 1090... Loss: 1.1421... Val Loss: 1.2548\n",
      "Epoch: 28... Step: 1100... Loss: 1.1339... Val Loss: 1.2592\n",
      "Epoch: 28... Step: 1110... Loss: 1.1494... Val Loss: 1.2551\n",
      "Epoch: 29... Step: 1120... Loss: 1.1448... Val Loss: 1.2550\n",
      "Epoch: 29... Step: 1130... Loss: 1.1657... Val Loss: 1.2550\n",
      "Epoch: 29... Step: 1140... Loss: 1.1709... Val Loss: 1.2543\n",
      "Epoch: 29... Step: 1150... Loss: 1.1777... Val Loss: 1.2561\n",
      "Epoch: 29... Step: 1160... Loss: 1.1347... Val Loss: 1.2571\n",
      "Epoch: 29... Step: 1170... Loss: 1.1550... Val Loss: 1.2475\n",
      "Epoch: 29... Step: 1180... Loss: 1.1439... Val Loss: 1.2501\n",
      "Epoch: 29... Step: 1190... Loss: 1.1783... Val Loss: 1.2542\n",
      "Epoch: 29... Step: 1200... Loss: 1.1359... Val Loss: 1.2554\n",
      "Epoch: 29... Step: 1210... Loss: 1.1366... Val Loss: 1.2596\n",
      "Epoch: 29... Step: 1220... Loss: 1.1519... Val Loss: 1.2533\n",
      "Epoch: 29... Step: 1230... Loss: 1.1371... Val Loss: 1.2557\n",
      "Epoch: 29... Step: 1240... Loss: 1.1392... Val Loss: 1.2590\n",
      "Epoch: 29... Step: 1250... Loss: 1.1589... Val Loss: 1.2562\n",
      "Epoch: 30... Step: 1260... Loss: 1.1437... Val Loss: 1.2573\n",
      "Epoch: 30... Step: 1270... Loss: 1.1569... Val Loss: 1.2538\n",
      "Epoch: 30... Step: 1280... Loss: 1.1628... Val Loss: 1.2518\n",
      "Epoch: 30... Step: 1290... Loss: 1.1498... Val Loss: 1.2561\n",
      "Epoch: 30... Step: 1300... Loss: 1.1389... Val Loss: 1.2526\n",
      "Epoch: 30... Step: 1310... Loss: 1.1536... Val Loss: 1.2490\n",
      "Epoch: 30... Step: 1320... Loss: 1.1269... Val Loss: 1.2491\n",
      "Epoch: 30... Step: 1330... Loss: 1.1264... Val Loss: 1.2506\n",
      "Epoch: 30... Step: 1340... Loss: 1.1146... Val Loss: 1.2494\n",
      "Epoch: 30... Step: 1350... Loss: 1.1135... Val Loss: 1.2566\n",
      "Epoch: 30... Step: 1360... Loss: 1.1255... Val Loss: 1.2517\n",
      "Epoch: 30... Step: 1370... Loss: 1.1214... Val Loss: 1.2505\n",
      "Epoch: 30... Step: 1380... Loss: 1.1633... Val Loss: 1.2515\n",
      "Epoch: 30... Step: 1390... Loss: 1.1956... Val Loss: 1.2468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31... Step: 1400... Loss: 1.1585... Val Loss: 1.2497\n",
      "Epoch: 31... Step: 1410... Loss: 1.1677... Val Loss: 1.2505\n",
      "Epoch: 31... Step: 1420... Loss: 1.1787... Val Loss: 1.2510\n",
      "Epoch: 31... Step: 1430... Loss: 1.1523... Val Loss: 1.2495\n",
      "Epoch: 31... Step: 1440... Loss: 1.1641... Val Loss: 1.2493\n",
      "Epoch: 31... Step: 1450... Loss: 1.1041... Val Loss: 1.2485\n",
      "Epoch: 31... Step: 1460... Loss: 1.1354... Val Loss: 1.2454\n",
      "Epoch: 31... Step: 1470... Loss: 1.1097... Val Loss: 1.2490\n",
      "Epoch: 31... Step: 1480... Loss: 1.1401... Val Loss: 1.2533\n",
      "Epoch: 31... Step: 1490... Loss: 1.1267... Val Loss: 1.2530\n",
      "Epoch: 31... Step: 1500... Loss: 1.1219... Val Loss: 1.2470\n",
      "Epoch: 31... Step: 1510... Loss: 1.1160... Val Loss: 1.2456\n",
      "Epoch: 31... Step: 1520... Loss: 1.1378... Val Loss: 1.2520\n",
      "Epoch: 32... Step: 1530... Loss: 1.2244... Val Loss: 1.2439\n",
      "Epoch: 32... Step: 1540... Loss: 1.1479... Val Loss: 1.2502\n",
      "Epoch: 32... Step: 1550... Loss: 1.1438... Val Loss: 1.2475\n",
      "Epoch: 32... Step: 1560... Loss: 1.1544... Val Loss: 1.2503\n",
      "Epoch: 32... Step: 1570... Loss: 1.1146... Val Loss: 1.2486\n",
      "Epoch: 32... Step: 1580... Loss: 1.1104... Val Loss: 1.2509\n",
      "Epoch: 32... Step: 1590... Loss: 1.1041... Val Loss: 1.2509\n",
      "Epoch: 32... Step: 1600... Loss: 1.1326... Val Loss: 1.2462\n",
      "Epoch: 32... Step: 1610... Loss: 1.1191... Val Loss: 1.2477\n",
      "Epoch: 32... Step: 1620... Loss: 1.1131... Val Loss: 1.2468\n",
      "Epoch: 32... Step: 1630... Loss: 1.1350... Val Loss: 1.2513\n",
      "Epoch: 32... Step: 1640... Loss: 1.1272... Val Loss: 1.2455\n",
      "Epoch: 32... Step: 1650... Loss: 1.1089... Val Loss: 1.2478\n",
      "Epoch: 32... Step: 1660... Loss: 1.1473... Val Loss: 1.2498\n",
      "Epoch: 33... Step: 1670... Loss: 1.1197... Val Loss: 1.2473\n",
      "Epoch: 33... Step: 1680... Loss: 1.1363... Val Loss: 1.2490\n",
      "Epoch: 33... Step: 1690... Loss: 1.1130... Val Loss: 1.2491\n",
      "Epoch: 33... Step: 1700... Loss: 1.1156... Val Loss: 1.2455\n",
      "Epoch: 33... Step: 1710... Loss: 1.1041... Val Loss: 1.2514\n",
      "Epoch: 33... Step: 1720... Loss: 1.1153... Val Loss: 1.2480\n",
      "Epoch: 33... Step: 1730... Loss: 1.1456... Val Loss: 1.2464\n",
      "Epoch: 33... Step: 1740... Loss: 1.1160... Val Loss: 1.2434\n",
      "Epoch: 33... Step: 1750... Loss: 1.0881... Val Loss: 1.2490\n",
      "Epoch: 33... Step: 1760... Loss: 1.1130... Val Loss: 1.2503\n",
      "Epoch: 33... Step: 1770... Loss: 1.1332... Val Loss: 1.2484\n",
      "Epoch: 33... Step: 1780... Loss: 1.1192... Val Loss: 1.2451\n",
      "Epoch: 33... Step: 1790... Loss: 1.1022... Val Loss: 1.2479\n",
      "Epoch: 33... Step: 1800... Loss: 1.1355... Val Loss: 1.2490\n",
      "Epoch: 34... Step: 1810... Loss: 1.1160... Val Loss: 1.2516\n",
      "Epoch: 34... Step: 1820... Loss: 1.1169... Val Loss: 1.2473\n",
      "Epoch: 34... Step: 1830... Loss: 1.1386... Val Loss: 1.2460\n",
      "Epoch: 34... Step: 1840... Loss: 1.0874... Val Loss: 1.2454\n",
      "Epoch: 34... Step: 1850... Loss: 1.0714... Val Loss: 1.2492\n",
      "Epoch: 34... Step: 1860... Loss: 1.1287... Val Loss: 1.2496\n",
      "Epoch: 34... Step: 1870... Loss: 1.1416... Val Loss: 1.2530\n",
      "Epoch: 34... Step: 1880... Loss: 1.1315... Val Loss: 1.2458\n",
      "Epoch: 34... Step: 1890... Loss: 1.1411... Val Loss: 1.2478\n",
      "Epoch: 34... Step: 1900... Loss: 1.1198... Val Loss: 1.2478\n",
      "Epoch: 34... Step: 1910... Loss: 1.1213... Val Loss: 1.2458\n",
      "Epoch: 34... Step: 1920... Loss: 1.1165... Val Loss: 1.2478\n",
      "Epoch: 34... Step: 1930... Loss: 1.0894... Val Loss: 1.2469\n",
      "Epoch: 34... Step: 1940... Loss: 1.1303... Val Loss: 1.2445\n",
      "Epoch: 35... Step: 1950... Loss: 1.1083... Val Loss: 1.2428\n",
      "Epoch: 35... Step: 1960... Loss: 1.1199... Val Loss: 1.2437\n",
      "Epoch: 35... Step: 1970... Loss: 1.0977... Val Loss: 1.2426\n",
      "Epoch: 35... Step: 1980... Loss: 1.1057... Val Loss: 1.2443\n",
      "Epoch: 35... Step: 1990... Loss: 1.0985... Val Loss: 1.2491\n",
      "Epoch: 35... Step: 2000... Loss: 1.0959... Val Loss: 1.2497\n",
      "Epoch: 35... Step: 2010... Loss: 1.1143... Val Loss: 1.2514\n",
      "Epoch: 35... Step: 2020... Loss: 1.1196... Val Loss: 1.2454\n",
      "Epoch: 35... Step: 2030... Loss: 1.0941... Val Loss: 1.2493\n",
      "Epoch: 35... Step: 2040... Loss: 1.1131... Val Loss: 1.2476\n",
      "Epoch: 35... Step: 2050... Loss: 1.1041... Val Loss: 1.2482\n",
      "Epoch: 35... Step: 2060... Loss: 1.1178... Val Loss: 1.2480\n",
      "Epoch: 35... Step: 2070... Loss: 1.1152... Val Loss: 1.2474\n",
      "Epoch: 35... Step: 2080... Loss: 1.1213... Val Loss: 1.2466\n",
      "Epoch: 36... Step: 2090... Loss: 1.1181... Val Loss: 1.2460\n",
      "Epoch: 36... Step: 2100... Loss: 1.1111... Val Loss: 1.2469\n",
      "Epoch: 36... Step: 2110... Loss: 1.1018... Val Loss: 1.2497\n",
      "Epoch: 36... Step: 2120... Loss: 1.1084... Val Loss: 1.2452\n",
      "Epoch: 36... Step: 2130... Loss: 1.0910... Val Loss: 1.2489\n",
      "Epoch: 36... Step: 2140... Loss: 1.1060... Val Loss: 1.2497\n",
      "Epoch: 36... Step: 2150... Loss: 1.1233... Val Loss: 1.2529\n",
      "Epoch: 36... Step: 2160... Loss: 1.1003... Val Loss: 1.2436\n",
      "Epoch: 36... Step: 2170... Loss: 1.1051... Val Loss: 1.2486\n",
      "Epoch: 36... Step: 2180... Loss: 1.1026... Val Loss: 1.2501\n",
      "Epoch: 36... Step: 2190... Loss: 1.1164... Val Loss: 1.2476\n",
      "Epoch: 36... Step: 2200... Loss: 1.1050... Val Loss: 1.2474\n",
      "Epoch: 36... Step: 2210... Loss: 1.0722... Val Loss: 1.2506\n",
      "Epoch: 36... Step: 2220... Loss: 1.1084... Val Loss: 1.2491\n",
      "Epoch: 37... Step: 2230... Loss: 1.1012... Val Loss: 1.2426\n",
      "Epoch: 37... Step: 2240... Loss: 1.1079... Val Loss: 1.2496\n",
      "Epoch: 37... Step: 2250... Loss: 1.0917... Val Loss: 1.2517\n",
      "Epoch: 37... Step: 2260... Loss: 1.1040... Val Loss: 1.2513\n",
      "Epoch: 37... Step: 2270... Loss: 1.0978... Val Loss: 1.2509\n",
      "Epoch: 37... Step: 2280... Loss: 1.1219... Val Loss: 1.2460\n",
      "Epoch: 37... Step: 2290... Loss: 1.1136... Val Loss: 1.2532\n",
      "Epoch: 37... Step: 2300... Loss: 1.0810... Val Loss: 1.2498\n",
      "Epoch: 37... Step: 2310... Loss: 1.0952... Val Loss: 1.2522\n",
      "Epoch: 37... Step: 2320... Loss: 1.0931... Val Loss: 1.2487\n",
      "Epoch: 37... Step: 2330... Loss: 1.0817... Val Loss: 1.2440\n",
      "Epoch: 37... Step: 2340... Loss: 1.0979... Val Loss: 1.2475\n",
      "Epoch: 37... Step: 2350... Loss: 1.1082... Val Loss: 1.2456\n",
      "Epoch: 37... Step: 2360... Loss: 1.1210... Val Loss: 1.2470\n",
      "Epoch: 38... Step: 2370... Loss: 1.0921... Val Loss: 1.2491\n",
      "Epoch: 38... Step: 2380... Loss: 1.0990... Val Loss: 1.2437\n",
      "Epoch: 38... Step: 2390... Loss: 1.0933... Val Loss: 1.2459\n",
      "Epoch: 38... Step: 2400... Loss: 1.0967... Val Loss: 1.2555\n",
      "Epoch: 38... Step: 2410... Loss: 1.1061... Val Loss: 1.2529\n",
      "Epoch: 38... Step: 2420... Loss: 1.1064... Val Loss: 1.2526\n",
      "Epoch: 38... Step: 2430... Loss: 1.1108... Val Loss: 1.2529\n",
      "Epoch: 38... Step: 2440... Loss: 1.0934... Val Loss: 1.2479\n",
      "Epoch: 38... Step: 2450... Loss: 1.0795... Val Loss: 1.2532\n",
      "Epoch: 38... Step: 2460... Loss: 1.1035... Val Loss: 1.2473\n",
      "Epoch: 38... Step: 2470... Loss: 1.0991... Val Loss: 1.2407\n",
      "Epoch: 38... Step: 2480... Loss: 1.0889... Val Loss: 1.2416\n",
      "Epoch: 38... Step: 2490... Loss: 1.0835... Val Loss: 1.2404\n",
      "Epoch: 38... Step: 2500... Loss: 1.0904... Val Loss: 1.2454\n",
      "Epoch: 39... Step: 2510... Loss: 1.0916... Val Loss: 1.2481\n",
      "Epoch: 39... Step: 2520... Loss: 1.1111... Val Loss: 1.2445\n",
      "Epoch: 39... Step: 2530... Loss: 1.1088... Val Loss: 1.2429\n",
      "Epoch: 39... Step: 2540... Loss: 1.1195... Val Loss: 1.2505\n",
      "Epoch: 39... Step: 2550... Loss: 1.0831... Val Loss: 1.2463\n",
      "Epoch: 39... Step: 2560... Loss: 1.0863... Val Loss: 1.2478\n",
      "Epoch: 39... Step: 2570... Loss: 1.0900... Val Loss: 1.2525\n",
      "Epoch: 39... Step: 2580... Loss: 1.1091... Val Loss: 1.2457\n",
      "Epoch: 39... Step: 2590... Loss: 1.0853... Val Loss: 1.2503\n",
      "Epoch: 39... Step: 2600... Loss: 1.0680... Val Loss: 1.2503\n",
      "Epoch: 39... Step: 2610... Loss: 1.0875... Val Loss: 1.2468\n",
      "Epoch: 39... Step: 2620... Loss: 1.0800... Val Loss: 1.2445\n",
      "Epoch: 39... Step: 2630... Loss: 1.0762... Val Loss: 1.2422\n",
      "Epoch: 39... Step: 2640... Loss: 1.1051... Val Loss: 1.2415\n",
      "Epoch: 40... Step: 2650... Loss: 1.0898... Val Loss: 1.2423\n",
      "Epoch: 40... Step: 2660... Loss: 1.1102... Val Loss: 1.2463\n",
      "Epoch: 40... Step: 2670... Loss: 1.1157... Val Loss: 1.2429\n",
      "Epoch: 40... Step: 2680... Loss: 1.0923... Val Loss: 1.2529\n",
      "Epoch: 40... Step: 2690... Loss: 1.0906... Val Loss: 1.2444\n",
      "Epoch: 40... Step: 2700... Loss: 1.1069... Val Loss: 1.2470\n",
      "Epoch: 40... Step: 2710... Loss: 1.0712... Val Loss: 1.2509\n",
      "Epoch: 40... Step: 2720... Loss: 1.0798... Val Loss: 1.2457\n",
      "Epoch: 40... Step: 2730... Loss: 1.0684... Val Loss: 1.2505\n",
      "Epoch: 40... Step: 2740... Loss: 1.0685... Val Loss: 1.2511\n",
      "Epoch: 40... Step: 2750... Loss: 1.0781... Val Loss: 1.2419\n",
      "Epoch: 40... Step: 2760... Loss: 1.0769... Val Loss: 1.2438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40... Step: 2770... Loss: 1.1082... Val Loss: 1.2434\n",
      "Epoch: 40... Step: 2780... Loss: 1.1383... Val Loss: 1.2422\n"
     ]
    }
   ],
   "source": [
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, \\\n",
    "           print_every=10, epoch_offset = 20,save_path = \"char_level_lstm.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"char_level_lstm.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we'll save the model so we can load it again later if we need to. Here, I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters, and the next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'char_level_lstm.net'\n",
    "\n",
    "checkpoint = {\n",
    "    'n_hidden': net.n_hidden,\n",
    "    'n_layer': net.n_layers,\n",
    "    'state_dict': net.state_dict(),\n",
    "    'tokens': net.chars\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('char_level_lstm.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded_net = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'],\\\n",
    "                     n_layers=checkpoint['n_layer'])\n",
    "loaded_net.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our RNN is from a fully-connected layer and it outputs a distribution of next-character scores. To actually get the next character, we apply a softmax function, which gives us a probability distribution that we can then sample to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2chars[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Levin said;!;NIS;EGU;YMGI7ER@UQV87`GU8@GU;IEGI7EG@I8EGSUISGU;GVQ(!EG7QSGUIN;G@;;7GU;YMGI7EGU;GVI@RI!!GSU;GPY87W8PI!G@Q(7EGQ7GSU;GWQ(7S87`GUQ(@;GQgGSU;GPYQN87W;MGI7EGSUISRSU8@GVI@GIG@SYI7`;GSUI7GI7uQ7;GVUQGVI@GIG@SY;7`SUMGSUISGU;GUIEGK;;7G@QG*(WURSU;G@I*;GS8*;GI7EGSU;GPY87W;@@MGVUQGUIEMGSU;G@SY;7`SUGQgGU8@GU;IYSRSUISGUIEGK;;7G@;;7GSUISGU;GVI@G@QG*(WUGIGWQ7@W8Q(@GSUISGU;GVQ(!EG7QSRUIN;GK;;7G@QG@8*P!;MGI7EGUIEG@Q*;SU87`G@QG*IYY8;EGSUISGU;GVI@G7QSGIG@;WQ7ERSQGU8@GV8g;MGI7EGSQGK;G@QG*IYYuI7`!uGSUISGSU;GPYQg;@@QYGVI@G7;N;YGI7`Y8!uRSQQGSU;G*Q@SG@QYSGQgGSY8`USGSUISGU;GUIEGK;`(7GSUISGSU;G@I*;G@QYSG87GSUISRPQ@8S8Q7GQgGSU;G@I*;MGSUISGSU8@GVI@GIG@87`!;G*I7GVUQGUI@GK;;7GI!!GISGQ7W;GSUISRU8@G@Q(!GVI@GIG!QN;!uG@UI*;GQgGSU;8YG@;7@;GQgGSU;G@I*;G@Q(!EGQgGS8*;%GAU;RPY87W;@@GV;Y;GIG@8!;7W;MGI7EGU;YGU(@KI7E @GgIW;GVI@G@SYI8`USGSQGSU;G@SIS;RQgGSU;G@SY;I*MGVU;Y;GSU;uGUIEGK;;7GIGWU8!EMGI7EGSUISGU;GUIEGK;;7G@QG@SYI8`USRSUISGU8@G@S(E8QGVI@GIG@87W;YGSQGU;I!GU8*@;!gMGSU;G*Q@SGPQYSYI8SMGSQGSU;GWQ(7S87`GUQ(@;RSU;G@I*;GS8*;GISGSU;G@I*;GS8*;GSQGK;GIG@;WQ7EGSU87`GSUISGU;GVQ(!EGUIN;RK;;7GIG!Q7`GVU8!;GSUISG@U;GVI@GIG@87`!;GQgGSUISGPYQN87`GSUISGUIEGK;;7RI!Y;IEuGSQG@IuGIG!8SS!;GQgGSU;G@SIYS87`GQgGSU;G@I*;G@SY;7`SUGQgGSU;GWQ(Y@;GQgRSU;G@SY;7`SUGQgGU8@GWUIY*MGI7EGU;GVI@G@QGI7`YuGV8SUGSUISGSU87`GQgR@(PPQYSGI7EGSU87`@GI7EG@Q*;SU87`GSUISGU;GVQ(!EGUIN;GK;;7GIG!8SS!;GQ7;GQgGSU;RPQYSYI8S@MGI7EGU;GUIEGK;;7GIG@8`78g8WI7W;GSQGU8*MGI7EGU;GUIEG7QSRSQ!EGU8*GSUISGU8@GVQY'@GVI@G@QGI@GSQG@;;GU8*MGI7EGUIEG@QG*(WUG@;;*;ERSQG@;;GU8*GI7EGU;YGU(@KI7EGI7EGUIEGK;;7GI!Y;IEuGISGSU;GWQ7E(WSQYMGI7EGSUISRSU87`@GV;Y;G@QG*(WUGI7EG@;SGSQG@IuMGSUISGSU;G@I*;G@SYI7`;GgIW;GVI@RIG@8`USGQgGSU;GPQYSYI8SGQgGSU;GPQ@8S8Q7GQgGSU;G@SIS8Q7GQgGSU;RWUIY*87`G@QYSGQgGSU87`@MGSUISGSU;uGUIEG7QSGSQGK;GIG!Q7`GVU8!;GI7EG@I8ERU;GVI@GI!!GSU;G@I*;GSU8@GVI@GSQG@IuGSUISGU;GVI@GIG!8SS!;GQgGSU;G@SIS;GQgRSU;G@SIS;@MGI7EGSQG@IuG@Q*;SU87`MGI7EGSUISGU;GVQ(!EG7QSGK;GI7uSU87`GSQGU;YR@Q7%RR.BUISGE8EGuQ(G@;;MGSU;GP;Yg;WSGQgGSU;G@I*;GSU87`GcG@IuGSQGU;Ym.RR.cGEQ7 SG'7QVGVUISGcG@UI!!\n"
     ]
    }
   ],
   "source": [
    "loaded_net.double()\n",
    "print(sample(loaded_net, 2000, prime='Anna Levin said', top_k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
