{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/text8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class TextPreProcessor:\n",
    "    def __init__(self, text):\n",
    "        self.text = text.lower()\n",
    "        #Replacing punctuation with tokens\n",
    "        self.text = self.text.replace('.',' <PERIOD> ')\n",
    "        self.text = self.text.replace(',', ' <COMMA> ')\n",
    "        self.text = self.text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "        self.text = self.text.replace(';', ' <SEMICOLON> ')\n",
    "        self.text = self.text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "        self.text = self.text.replace('?', ' <QUESTION_MARK> ')\n",
    "        self.text = self.text.replace('(', ' <LEFT_PAREN> ')\n",
    "        self.text = self.text.replace(')', ' <RIGHT_PAREN> ')\n",
    "        self.text = self.text.replace('--', ' <HYPHENS> ')\n",
    "        self.text = self.text.replace('?', ' <QUESTION_MARK> ')\n",
    "        self.text = self.text.replace(':', ' <COLON> ')\n",
    "        \n",
    "        self.words = self.text.split()\n",
    "        self.word_counts = Counter(self.words)\n",
    "        \n",
    "        \n",
    "    def get_trimmed_words(self, min_word_occ):\n",
    "        return [word for word in self.words if self.word_counts[word] > min_word_occ]\n",
    "    \n",
    "    def create_lookup_tables(self):\n",
    "        sorted_vocab = sorted(self.word_counts, key=self.word_counts.get, reverse=True)\n",
    "        \n",
    "        int_to_vocab = {i:v for i,v in enumerate(sorted_vocab)}\n",
    "        vocab_to_int = {v:i for i,v in int_to_vocab.items()}\n",
    "\n",
    "        return vocab_to_int, int_to_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pre_processor = TextPreProcessor(text)\n",
    "words = text_pre_processor.get_trimmed_words(5)\n",
    "vocab_to_int, int_to_vocab = text_pre_processor.create_lookup_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'including',\n",
       " 'the',\n",
       " 'diggers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'english',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sans',\n",
       " 'culottes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'french',\n",
       " 'revolution',\n",
       " 'whilst']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in text: 16680599\n",
      "Total unique words in text: 63641\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in text: {}\".format(len(words)))\n",
    "print(\"Total unique words in text: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'the'),\n",
       " (1, 'of'),\n",
       " (2, 'and'),\n",
       " (3, 'one'),\n",
       " (4, 'in'),\n",
       " (5, 'a'),\n",
       " (6, 'to'),\n",
       " (7, 'zero'),\n",
       " (8, 'nine'),\n",
       " (9, 'two'),\n",
       " (10, 'is'),\n",
       " (11, 'as'),\n",
       " (12, 'eight'),\n",
       " (13, 'for'),\n",
       " (14, 's'),\n",
       " (15, 'five'),\n",
       " (16, 'three'),\n",
       " (17, 'was'),\n",
       " (18, 'by'),\n",
       " (19, 'that')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(int_to_vocab.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " ('of', 1),\n",
       " ('and', 2),\n",
       " ('one', 3),\n",
       " ('in', 4),\n",
       " ('a', 5),\n",
       " ('to', 6),\n",
       " ('zero', 7),\n",
       " ('nine', 8),\n",
       " ('two', 9),\n",
       " ('is', 10),\n",
       " ('as', 11),\n",
       " ('eight', 12),\n",
       " ('for', 13),\n",
       " ('s', 14),\n",
       " ('five', 15),\n",
       " ('three', 16),\n",
       " ('was', 17),\n",
       " ('by', 18),\n",
       " ('that', 19)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab_to_int.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that show up often such as \"the\", \"of\", and \"for\" don't provide much context to the nearby words. If we discard some of them, we can remove some of the noise from our data and in return get faster training and better representations. This process is called subsampling by Mikolov. For each word $w_i$ in the training set, we'll discard it with probability given by \n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "where $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "\n",
    "total_count = len(words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1-np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "\n",
    "train_words = [word for word in int_words if random.random() > p_drop[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism\n",
      "originated\n",
      "term\n",
      "abuse\n",
      "early\n",
      "radicals\n",
      "diggers\n",
      "english\n",
      "the\n",
      "sans\n"
     ]
    }
   ],
   "source": [
    "for i in train_words[:10]:\n",
    "    print(int_to_vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Batches\n",
    "With the skip-gram architecture, for each word in the text, we want to define a surrounding _context_ and grab all the words in a window around that word, with size $C$. \n",
    "\n",
    "From [Mikolov et al.](https://arxiv.org/pdf/1301.3781.pdf): \n",
    "\n",
    "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C = 5$, for each training word we will select randomly a number $R$ in range $[ 1: C ]$, and then use $R$ words from history and $R$ words from the future of the current word as correct labels.\"\n",
    "\n",
    "Say, we have an input and we're interested in the idx=2 token, `741`: \n",
    "```\n",
    "[5233, 58, 741, 10571, 27349, 0, 15067, 58112, 3580, 58, 10712]\n",
    "```\n",
    "\n",
    "For `R=2`, `get_target` should return a list of four values:\n",
    "```\n",
    "[5233, 58, 10571, 27349]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size = 5):\n",
    "    \n",
    "    r = np.random.randint(1, window_size+1)\n",
    "    start = idx - r if idx>=r else 0\n",
    "    stop = idx + r\n",
    "    \n",
    "    return words[start:idx] + words[idx+1:stop+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Target:  [3, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# test your code!\n",
    "\n",
    "# run this cell multiple times to check for random window selection\n",
    "int_text = [i for i in range(10)]\n",
    "print('Input: ', int_text)\n",
    "idx=5 # word index of interest\n",
    "\n",
    "target = get_target(int_text, idx=idx, window_size=2)\n",
    "print('Target: ', target)  # you should get some indices around the idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Batches\n",
    "Here's a generator functions that returns batches of input and target data for the model, using the `get_target` functino from above. The idea is that it grabs `batch_size` worods fromo a words list. Then for each of those batches, it gets the target words in a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size = 5):\n",
    "    \n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [],[]\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "        yield x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n",
      "y\n",
      " [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "int_text = [i for i in range(20)]\n",
    "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
    "\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "We need a function to help us observe the model as it learns. We'll use cosine similarity to measure closness of words:\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
    "$$\n",
    "\n",
    "We can encode the validation words as vectors $\\vec{a}$ using the embedding table, then calculate the similarity with each vector $\\vec{b}$ in the embedding table. This way, we can print out the validation words and words in our embedding table semantically similar to those words. It's a nice way to check that our embedding table is grouping together words with similar semantic meanings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding, valid_size=16, valid_window=100, device = 'cpu'):\n",
    "    '''\n",
    "        Returns the cosine similarity of validation words with words in the embedding matrix.\n",
    "        Here, embedding should be a PyTorch embedding module\n",
    "    '''\n",
    "    # Here, we're calculating the cosine similarity between some random words and\n",
    "    #our embedding vectors. With the similarities, we can look at waht words are \n",
    "    # close to our random words\n",
    "    \n",
    "    # sim (a.b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    #magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from ranges (0, window) and (1000, 1000+window)\n",
    "    # Lower id implies more frequent\n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \\\n",
    "                               random.sample(range(1000, 1000+valid_window), valid_size//2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
    "    \n",
    "    return valid_examples, similarities\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Negative Sampling\n",
    "\n",
    "For every example we give the network, we train it using the output from the softmax layer. That means for each input, we're making very small changes to millions of weights even though we only have one true example. This makes training the network very inefficient. We can approximate the loss from the softmax layer by only updating a small subset of all the weights at once. We'll update the weights for the correct example, but only a small number of incorrect, or noise, examples. This is called [\"negative sampling\"](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). \n",
    "\n",
    "There are two modifications we need to make. First, since we're not taking the softmax output over all the words, we're really only concerned with one output word at a time. Similar to how we use an embedding table to map the input word to the hidden layer, we can now use another embedding table to map the hidden layer to the output word. Now we have two embedding layers, one for input words and one for output words. Secondly, we use a modified loss function where we only care about the true example and a small subset of noise examples.\n",
    "\n",
    "$$\n",
    "- \\large \\log{\\sigma\\left(u_{w_O}\\hspace{0.001em}^\\top v_{w_I}\\right)} -\n",
    "\\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}\\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)}\n",
    "$$\n",
    "\n",
    "This is a little complicated so I'll go through it bit by bit. $u_{w_O}\\hspace{0.001em}^\\top$ is the embedding vector for our \"output\" target word (transposed, that's the $^\\top$ symbol) and $v_{w_I}$ is the embedding vector for the \"input\" word. Then the first term \n",
    "\n",
    "$$\\large \\log{\\sigma\\left(u_{w_O}\\hspace{0.001em}^\\top v_{w_I}\\right)}$$\n",
    "\n",
    "says we take the log-sigmoid of the inner product of the output word vector and the input word vector. Now the second term, let's first look at \n",
    "\n",
    "$$\\large \\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}$$ \n",
    "\n",
    "This means we're going to take a sum over words $w_i$ drawn from a noise distribution $w_i \\sim P_n(w)$. The noise distribution is basically our vocabulary of words that aren't in the context of our input word. In effect, we can randomly sample words from our vocabulary to get these words. $P_n(w)$ is an arbitrary probability distribution though, which means we get to decide how to weight the words that we're sampling. This could be a uniform distribution, where we sample all words with equal probability. Or it could be according to the frequency that each word shows up in our text corpus, the unigram distribution $U(w)$. The authors found the best distribution to be $U(w)^{3/4}$, empirically. \n",
    "\n",
    "Finally, in \n",
    "\n",
    "$$\\large \\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)},$$ \n",
    "\n",
    "we take the log-sigmoid of the negated inner product of a noise vector with the input vector. \n",
    "\n",
    "To give you an intuition for what we're doing here, remember that the sigmoid function returns a probability between 0 and 1. The first term in the loss pushes the probability that our network will predict the correct word $w_O$ towards 1. In the second term, since we are negating the sigmoid input, we're pushing the probabilities of the noise words towards 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab\n",
    "        self.n_embed = n_embed\n",
    "        self.noise_dist = noise_dist\n",
    "        \n",
    "        #embedding layers for input and output words\n",
    "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
    "        \n",
    "        #initializing embedding tables with uniform distribution\n",
    "        self.in_embed.weight.data.uniform_(-1,1)\n",
    "        self.out_embed.weight.data.uniform_(-1,1)\n",
    "        \n",
    "    def forward_input(self, input_words):\n",
    "        input_vectors = self.in_embed(input_words)\n",
    "        return input_vectors\n",
    "    \n",
    "    def forward_output(self, output_words):\n",
    "        output_vectors = self.out_embed(output_words)\n",
    "        return output_vectors\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\"\n",
    "        Generate noise vectors with shape (batch_size, n_samples, n_embed)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.noise_dist is None:\n",
    "            noise_dist = torch.ones(self.n_vocab)\n",
    "        else:\n",
    "            noise_dist = self.noise_dist\n",
    "            \n",
    "        noise_words = torch.multinomial(noise_dist, batch_size*n_samples, replacement=True)\n",
    "        \n",
    "        device = \"cuda\" if model.out_embed.weight.is_cuda else \"cpu\"\n",
    "        noise_words = noise_words.to(device)\n",
    "        \n",
    "        noise_vectors = self.out_embed(noise_words).view(batch_size, n_samples, self.n_embed)\n",
    "        \n",
    "        return noise_vectors\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        \n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # Input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # Output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # correct log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        out_loss = out_loss.squeeze()\n",
    "        \n",
    "        # incorrect log-sigmoid loss\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
    "\n",
    "        # negate and sum correct and noisy log-sigmoid losses\n",
    "        # return average batch loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25\n",
      "Loss:  6.714881420135498\n",
      "be | one, number, in, picta, doctorates\n",
      "by | comnenus, caesar, desperate, of, a\n",
      "from | an, neoconservatism, of, kevin, entrusted\n",
      "united | have, judgment, munch, angle, publicized\n",
      "its | blues, toxicity, recrystallized, tobago, pareudiastes\n",
      "all | monthly, reverdy, grapefruits, experiments, ellipsoids\n",
      "d | potomac, much, ferment, location, rayok\n",
      "zero | two, nine, fights, chief, of\n",
      "question | nominations, vacuums, ortelius, polarized, thecommonreview\n",
      "older | cois, fartknocker, rufifacies, thomond, allose\n",
      "mainly | etorri, apparently, oxygen, favour, irritate\n",
      "resources | autobiography, collaborators, due, foulenough, rca\n",
      "units | dio, brihadaranyaka, athlete, inactivation, sock\n",
      "road | galloping, park, studio, miami, tourism\n",
      "event | morrisette, boulevards, rioth, hitchcocks, boilers\n",
      "engine | firmicutes, hypoxidaceae, bsar, dwarfish, ius\n",
      "...\n",
      "\n",
      "Epoch: 1/25\n",
      "Loss:  5.272209644317627\n",
      "more | the, a, are, in, lighthouse\n",
      "most | and, the, in, for, to\n",
      "can | the, one, in, and, zero\n",
      "zero | of, the, nine, in, four\n",
      "while | nine, the, to, buechner, one\n",
      "in | of, the, and, one, to\n",
      "into | in, the, is, areas, duras\n",
      "states | there, was, bacterial, into, sulfides\n",
      "issue | medell, sudden, observed, coconut, crumble\n",
      "articles | malcom, kathakali, minutes, what, fresnay\n",
      "derived | in, caliburn, claffheim, the, triangular\n",
      "numerous | active, canals, serving, ornaments, good\n",
      "taking | english, anonymously, cal, hedley, recorded\n",
      "magazine | trip, modern, gregory, first, developed\n",
      "test | under, unregistered, buys, krzyzewski, zero\n",
      "freedom | valuing, godly, gemss, colonies, each\n",
      "...\n",
      "\n",
      "Epoch: 1/25\n",
      "Loss:  4.349923133850098\n",
      "will | the, that, is, there, or\n",
      "many | as, the, that, for, of\n",
      "all | is, a, the, be, an\n",
      "during | first, his, in, of, and\n",
      "and | of, the, in, is, be\n",
      "from | of, to, the, s, by\n",
      "an | a, the, of, is, be\n",
      "where | a, can, an, with, the\n",
      "square | of, at, zero, singular, there\n",
      "universe | disfigured, monomer, peptidoglycans, notoriety, shaw\n",
      "behind | but, were, s, bumps, astroturf\n",
      "account | number, delight, standout, elaborate, sinks\n",
      "versions | lasker, boehmite, links, proofs, allzugleich\n",
      "file | lists, switzer, array, again, seska\n",
      "police | la, fired, mexican, surfaces, richard\n",
      "primarily | reconquered, abbasid, morality, retroactive, opening\n",
      "...\n",
      "\n",
      "Epoch: 1/25\n",
      "Loss:  3.7616078853607178\n",
      "of | and, the, in, was, from\n",
      "about | by, it, and, while, of\n",
      "such | by, be, can, a, the\n",
      "some | that, a, and, of, be\n",
      "in | the, and, of, was, one\n",
      "while | and, to, about, other, the\n",
      "most | the, and, of, not, are\n",
      "however | to, most, the, but, of\n",
      "file | seska, system, tarsians, lists, array\n",
      "liberal | pain, ideological, spinal, clergy, member\n",
      "bible | god, christian, christianity, jesus, announce\n",
      "centre | dwight, tranquility, market, world, snores\n",
      "creation | alghamdi, the, were, altamira, cururu\n",
      "numerous | by, poverty, despite, its, amato\n",
      "defense | legal, improvisationally, brucite, expels, cold\n",
      "accepted | must, vain, nietzsche, mismatched, never\n",
      "...\n",
      "\n",
      "Epoch: 1/25\n",
      "Loss:  3.2693932056427\n",
      "six | three, nine, five, one, seven\n",
      "years | two, one, zero, six, three\n",
      "the | of, as, and, to, in\n",
      "i | you, him, on, if, the\n",
      "into | to, and, as, the, of\n",
      "such | to, can, be, a, for\n",
      "however | after, of, to, was, the\n",
      "which | and, as, of, to, a\n",
      "san | hugo, one, factionalist, california, baltzersen\n",
      "instance | meaning, garciaparra, difficult, mathematical, girardeau\n",
      "woman | her, featuring, novel, play, you\n",
      "assembly | party, president, united, elections, council\n",
      "hit | producer, movie, hall, player, get\n",
      "experience | doctrine, understood, ibsen, centre, having\n",
      "award | best, film, team, final, rock\n",
      "discovered | she, her, mtbe, often, usa\n",
      "...\n",
      "\n",
      "Epoch: 1/25\n",
      "Loss:  3.369598150253296\n",
      "see | in, is, links, language, external\n",
      "one | five, nine, three, four, two\n",
      "has | for, the, all, other, its\n",
      "war | forces, battle, against, army, during\n",
      "as | to, a, the, in, of\n",
      "on | with, s, in, of, after\n",
      "united | states, federal, national, government, city\n",
      "system | systems, operating, user, application, code\n",
      "bible | testament, jesus, hebrew, christianity, christian\n",
      "animals | food, animal, often, species, many\n",
      "troops | army, forces, soviet, war, occupation\n",
      "issue | political, organization, legislation, democratic, party\n",
      "freedom | political, liberty, economic, congress, christians\n",
      "taking | individuals, can, be, kifl, have\n",
      "notes | musical, text, using, characters, this\n",
      "joseph | william, b, american, john, writer\n",
      "...\n",
      "\n",
      "Epoch: 2/25\n",
      "Loss:  2.8010354042053223\n",
      "into | the, of, was, and, in\n",
      "while | any, it, usually, to, often\n",
      "years | population, age, under, five, female\n",
      "where | the, which, its, is, of\n",
      "s | by, nine, first, one, was\n",
      "have | are, that, they, is, and\n",
      "often | most, including, are, usually, or\n",
      "over | the, land, at, years, between\n",
      "event | run, victory, murder, won, play\n",
      "something | our, that, nature, so, we\n",
      "engine | powered, engines, speed, models, car\n",
      "taking | had, by, a, and, money\n",
      "mainly | its, southern, north, region, parts\n",
      "frac | y, cdot, defined, equation, vector\n",
      "stage | contact, also, a, find, put\n",
      "arts | teacher, school, art, trained, martial\n",
      "...\n",
      "\n",
      "Epoch: 2/25\n",
      "Loss:  2.561821937561035\n",
      "not | must, can, any, to, does\n",
      "be | if, is, not, to, person\n",
      "or | from, is, as, usually, of\n",
      "as | the, in, from, a, also\n",
      "one | five, four, nine, two, six\n",
      "two | five, one, three, six, four\n",
      "between | from, of, large, there, have\n",
      "war | forces, army, soviet, armed, invasion\n",
      "prince | king, queen, wife, married, charles\n",
      "orthodox | catholic, orthodoxy, catholics, jews, churches\n",
      "taking | the, to, a, then, for\n",
      "grand | duke, islands, town, germany, island\n",
      "test | tests, normal, cost, when, assessment\n",
      "animals | animal, eating, species, prey, fish\n",
      "something | things, this, doing, goes, go\n",
      "pressure | temperature, effect, reduce, heating, surface\n",
      "...\n",
      "\n",
      "Epoch: 2/25\n",
      "Loss:  2.6900110244750977\n",
      "will | that, should, do, is, be\n",
      "his | he, himself, him, was, died\n",
      "eight | one, six, nine, three, seven\n",
      "known | the, of, which, in, a\n",
      "not | be, does, if, do, that\n",
      "been | to, have, because, or, of\n",
      "many | are, most, as, often, including\n",
      "is | the, for, this, as, are\n",
      "magazine | published, media, movie, comic, interview\n",
      "versions | windows, version, software, create, interface\n",
      "event | extinction, observed, mass, drift, will\n",
      "test | tests, prevention, systems, procedures, change\n",
      "arts | school, college, university, cambridge, art\n",
      "smith | john, alan, joseph, books, jr\n",
      "egypt | sudan, arab, egyptian, iran, muslim\n",
      "prince | emperor, son, throne, imperial, king\n",
      "...\n",
      "\n",
      "Epoch: 2/25\n",
      "Loss:  2.830986738204956\n",
      "only | all, be, to, or, any\n",
      "or | with, be, is, called, not\n",
      "an | and, by, to, of, s\n",
      "about | account, historians, many, historical, two\n",
      "when | to, end, out, his, and\n",
      "has | is, of, with, a, and\n",
      "d | b, l, nine, r, j\n",
      "known | by, the, of, and, from\n",
      "quite | not, because, more, different, so\n",
      "paris | de, la, france, pierre, jean\n",
      "file | files, software, microsoft, unix, windows\n",
      "additional | each, can, simple, common, cases\n",
      "pope | catholic, emperor, viii, bishops, holy\n",
      "consists | legislative, presidential, consisting, regional, map\n",
      "mathematics | mathematical, theory, analysis, scientific, philosophical\n",
      "mainly | india, peoples, agricultural, predominant, france\n",
      "...\n",
      "\n",
      "Epoch: 2/25\n",
      "Loss:  2.841674327850342\n",
      "where | the, into, on, be, a\n",
      "it | with, the, is, to, be\n",
      "all | the, as, for, only, or\n",
      "however | are, be, their, but, not\n",
      "other | and, are, often, some, from\n",
      "be | because, not, or, to, person\n",
      "s | one, in, six, the, nine\n",
      "for | the, or, as, and, on\n",
      "police | military, criminal, forces, troops, officers\n",
      "troops | war, forces, army, fought, attack\n",
      "arts | art, school, martial, styles, cambridge\n",
      "engineering | technology, engineers, electronics, computing, design\n",
      "universe | cosmology, concept, nature, cosmic, evil\n",
      "except | the, it, used, is, are\n",
      "articles | article, see, org, links, website\n",
      "lived | children, whom, died, born, wife\n",
      "...\n",
      "\n",
      "Epoch: 2/25\n",
      "Loss:  2.885565757751465\n",
      "about | and, of, s, earth, has\n",
      "been | sometimes, has, very, have, were\n",
      "american | actor, nine, politician, writer, b\n",
      "other | are, or, such, common, some\n",
      "with | the, as, to, was, in\n",
      "used | use, applications, standard, etc, using\n",
      "often | create, these, are, or, some\n",
      "more | be, such, with, other, however\n",
      "derived | languages, see, texts, word, latin\n",
      "shown | properties, description, below, table, above\n",
      "operating | unix, software, os, systems, windows\n",
      "primarily | include, are, modern, own, variety\n",
      "account | caused, nature, sources, life, truth\n",
      "woman | her, women, death, lady, birth\n",
      "experience | virtue, experiences, credibility, psychological, influence\n",
      "http | www, links, com, org, html\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/25\n",
      "Loss:  2.679097890853882\n",
      "his | he, him, brother, was, son\n",
      "see | of, also, the, five, article\n",
      "often | or, tend, addition, wide, can\n",
      "history | links, references, list, encyclopedia, external\n",
      "he | his, him, himself, had, brother\n",
      "will | that, should, you, me, a\n",
      "with | a, in, and, as, the\n",
      "for | a, also, set, and, or\n",
      "derived | latin, derivation, meaning, name, languages\n",
      "additional | instead, file, example, standard, for\n",
      "running | run, runs, coach, game, introduced\n",
      "prince | king, emperor, duke, queen, reign\n",
      "instance | example, substance, valid, be, define\n",
      "recorded | rock, songs, band, album, music\n",
      "operations | operation, group, civilian, addition, supplies\n",
      "scale | scales, relative, instruments, tone, markedly\n",
      "...\n",
      "\n",
      "Epoch: 3/25\n",
      "Loss:  2.4551072120666504\n",
      "so | can, to, too, could, they\n",
      "between | from, the, an, are, of\n",
      "were | was, during, destroyed, armies, had\n",
      "was | his, after, had, the, who\n",
      "their | the, they, a, and, more\n",
      "used | use, commonly, or, are, common\n",
      "during | period, to, was, in, the\n",
      "its | is, the, from, and, of\n",
      "account | historian, according, ancient, says, sources\n",
      "taking | dangerous, the, a, out, when\n",
      "lived | his, century, died, tribes, survived\n",
      "recorded | songs, most, month, album, including\n",
      "defense | defensive, staff, combat, defence, linebacker\n",
      "older | age, males, median, household, families\n",
      "freedom | political, economic, liberty, freedoms, communism\n",
      "orthodox | catholic, churches, church, christianity, christian\n",
      "...\n",
      "\n",
      "Epoch: 3/25\n",
      "Loss:  2.7424888610839844\n",
      "an | is, a, of, the, be\n",
      "states | united, nations, independence, monetary, republic\n",
      "only | is, of, the, are, no\n",
      "such | and, example, or, is, often\n",
      "world | economic, europe, nations, most, italy\n",
      "all | are, any, can, complete, be\n",
      "may | is, can, or, for, cannot\n",
      "zero | two, three, four, one, five\n",
      "marriage | married, her, marriages, daughter, marry\n",
      "file | files, linux, executable, interface, format\n",
      "creation | evolution, creationism, universe, humanity, idea\n",
      "units | unit, measured, si, density, energy\n",
      "egypt | arab, egyptian, cairo, palestine, saudi\n",
      "institute | university, research, college, technology, school\n",
      "defense | military, defence, combat, personnel, intelligence\n",
      "report | reports, review, committee, regarding, compliance\n",
      "...\n",
      "\n",
      "Epoch: 3/25\n",
      "Loss:  2.3293826580047607\n",
      "a | and, is, of, first, as\n",
      "who | was, whom, him, refused, his\n",
      "he | his, him, was, she, attended\n",
      "five | three, four, six, zero, eight\n",
      "will | if, can, adds, every, are\n",
      "d | b, l, footballer, j, writer\n",
      "system | systems, interface, implemented, unix, computers\n",
      "nine | eight, one, seven, four, five\n",
      "quite | because, but, might, these, it\n",
      "shows | show, television, viewers, tv, best\n",
      "marriage | wife, heir, husband, daughter, divorce\n",
      "universe | cosmology, cosmic, cosmological, planets, earth\n",
      "numerous | many, various, original, traditions, traditional\n",
      "smith | john, robert, barry, writer, davis\n",
      "professional | football, teams, hockey, athletic, amateur\n",
      "behind | over, leading, looked, impressive, keeps\n",
      "...\n",
      "\n",
      "Epoch: 3/25\n",
      "Loss:  2.3134655952453613\n",
      "been | the, that, including, as, many\n",
      "during | war, was, period, after, the\n",
      "years | months, year, birth, age, died\n",
      "than | more, usually, or, often, can\n",
      "five | zero, three, six, two, four\n",
      "first | was, he, had, his, eight\n",
      "not | that, it, as, with, their\n",
      "they | them, it, there, when, their\n",
      "centre | town, helsinki, cities, situated, city\n",
      "stage | film, filming, roles, hollywood, plays\n",
      "institute | research, university, college, engineering, science\n",
      "construction | built, transportation, building, passenger, buildings\n",
      "award | awards, best, academy, emmy, hugo\n",
      "active | the, members, community, from, formation\n",
      "egypt | egyptian, arab, sudan, libya, syria\n",
      "cost | costs, premium, revenue, phased, overhead\n",
      "...\n",
      "\n",
      "Epoch: 3/25\n",
      "Loss:  2.378908157348633\n",
      "the | of, in, and, to, only\n",
      "only | the, to, are, of, is\n",
      "about | might, of, are, to, in\n",
      "new | york, first, company, zero, named\n",
      "that | to, as, be, not, rather\n",
      "to | the, that, be, a, in\n",
      "this | is, with, often, to, be\n",
      "have | are, to, their, more, they\n",
      "recorded | songs, album, recording, featuring, tour\n",
      "arts | martial, art, practitioners, styles, schools\n",
      "troops | army, forces, soldiers, war, invasion\n",
      "centre | town, manchester, london, city, university\n",
      "hold | be, that, not, even, opponents\n",
      "account | caused, conclusions, attributed, believe, death\n",
      "universe | cosmological, evil, beings, universes, cosmos\n",
      "http | www, com, org, html, edu\n",
      "...\n",
      "\n",
      "Epoch: 4/25\n",
      "Loss:  2.149747848510742\n",
      "four | two, one, six, three, seven\n",
      "people | many, americans, their, themselves, men\n",
      "zero | two, five, one, three, four\n",
      "an | a, which, see, by, the\n",
      "for | and, to, a, which, on\n",
      "new | york, s, nine, first, university\n",
      "is | the, a, are, in, or\n",
      "such | products, properties, component, used, many\n",
      "paris | sur, de, des, jacques, france\n",
      "bbc | listing, news, broadcasting, links, april\n",
      "except | from, the, territory, it, a\n",
      "notes | bass, norman, page, note, harp\n",
      "road | roads, built, park, opened, town\n",
      "freedom | views, liberty, political, freedoms, deny\n",
      "older | females, males, families, median, years\n",
      "ice | winter, snow, waters, arctic, dark\n",
      "...\n",
      "\n",
      "Epoch: 4/25\n",
      "Loss:  2.405698299407959\n",
      "many | some, are, people, as, other\n",
      "were | had, was, them, in, who\n",
      "can | be, either, is, cannot, normal\n",
      "first | the, in, second, one, series\n",
      "as | the, in, also, and, not\n",
      "history | links, external, com, list, references\n",
      "have | are, more, to, as, that\n",
      "will | to, must, because, can, you\n",
      "taking | to, costly, their, by, and\n",
      "except | separate, each, territory, to, the\n",
      "assembly | parliamentary, president, council, elections, party\n",
      "running | run, quick, end, flash, runs\n",
      "numerous | in, as, various, association, are\n",
      "arts | martial, college, school, university, education\n",
      "issue | controversy, opinions, debate, book, construing\n",
      "nobel | prize, laureate, physicist, chemist, politician\n",
      "...\n",
      "\n",
      "Epoch: 4/25\n",
      "Loss:  2.596836805343628\n",
      "is | of, the, can, an, are\n",
      "american | politician, musician, born, prize, actor\n",
      "system | systems, is, allows, can, simple\n",
      "after | was, had, last, died, he\n",
      "at | of, on, new, a, the\n",
      "use | used, include, similar, tools, uses\n",
      "had | was, him, he, his, died\n",
      "are | is, or, many, different, these\n",
      "older | families, median, age, household, males\n",
      "ice | frozen, snow, melting, winter, hockey\n",
      "resources | resource, agriculture, exploitation, arable, economic\n",
      "bbc | news, television, itv, radio, february\n",
      "defense | defence, policy, department, military, alleged\n",
      "cost | costs, dollars, revenue, quality, profits\n",
      "lived | mother, died, believed, centuries, moors\n",
      "professional | sports, association, amateur, professionals, qualified\n",
      "...\n",
      "\n",
      "Epoch: 4/25\n",
      "Loss:  2.4283339977264404\n",
      "but | it, this, the, to, they\n",
      "between | the, and, this, divided, following\n",
      "to | it, and, that, their, the\n",
      "however | it, this, not, that, the\n",
      "i | you, my, go, me, if\n",
      "would | be, this, that, it, did\n",
      "who | to, of, was, he, appointed\n",
      "about | zero, five, two, account, million\n",
      "award | awards, hugo, best, won, awarded\n",
      "bible | tanakh, testament, gospel, prophets, translations\n",
      "older | median, age, males, families, zero\n",
      "proposed | proposals, european, independently, theory, discoveries\n",
      "resources | tools, resource, products, mineral, information\n",
      "dr | director, john, j, frank, university\n",
      "issue | illegal, policies, debate, government, allegations\n",
      "question | questions, truth, answer, we, questioned\n",
      "...\n",
      "\n",
      "Epoch: 4/25\n",
      "Loss:  2.545630931854248\n",
      "system | systems, provide, developed, functions, device\n",
      "th | three, century, nd, seven, eight\n",
      "is | the, a, for, of, or\n",
      "history | links, of, timeline, references, see\n",
      "by | of, the, in, s, as\n",
      "united | states, u, nations, president, state\n",
      "often | including, and, sometimes, especially, however\n",
      "world | war, in, nine, see, one\n",
      "channel | channels, network, radio, cable, signal\n",
      "operations | personnel, operation, naval, carrier, logistics\n",
      "grand | prix, park, seat, municipal, crown\n",
      "shown | ptilodus, exactly, symbol, fast, contains\n",
      "bible | biblical, tanakh, testament, torah, translations\n",
      "frac | cdot, cos, equation, cdots, sqrt\n",
      "animals | animal, humans, mammals, species, eat\n",
      "troops | forces, army, defeat, war, surrender\n",
      "...\n",
      "\n",
      "Epoch: 4/25\n",
      "Loss:  2.478412389755249\n",
      "up | a, this, into, or, to\n",
      "often | used, can, these, as, sometimes\n",
      "they | be, them, their, to, these\n",
      "united | states, commonwealth, uruguay, nations, state\n",
      "from | and, also, of, the, to\n",
      "first | the, was, in, and, after\n",
      "one | five, nine, seven, four, eight\n",
      "eight | six, seven, nine, one, four\n",
      "gold | silver, coins, ore, copper, bronze\n",
      "question | answer, questions, understand, what, true\n",
      "ice | hockey, winter, snow, frozen, glaciers\n",
      "professional | organization, baseball, amateur, management, academic\n",
      "mathematics | mathematical, geometry, algebra, mathematicians, theorems\n",
      "account | cannot, fact, billions, accounted, to\n",
      "test | tests, testing, nuclear, flight, cricket\n",
      "additional | can, provides, require, number, systems\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/25\n",
      "Loss:  2.242072343826294\n",
      "new | york, seven, company, edition, one\n",
      "been | have, was, to, the, a\n",
      "than | usually, are, to, it, more\n",
      "there | have, the, many, are, in\n",
      "th | century, nd, rd, centuries, st\n",
      "after | was, he, last, had, during\n",
      "s | by, nine, the, and, was\n",
      "over | from, to, years, in, the\n",
      "articles | org, article, http, links, publications\n",
      "http | www, com, html, htm, org\n",
      "notes | note, diatonic, isbn, tuning, chromatic\n",
      "engine | engines, powered, combustion, diesel, jet\n",
      "derived | modern, meaning, name, derivation, word\n",
      "ocean | atlantic, pacific, islands, sea, mountains\n",
      "magazine | monthly, magazines, fan, published, video\n",
      "freedom | liberty, political, anti, equality, freedoms\n",
      "...\n",
      "\n",
      "Epoch: 5/25\n",
      "Loss:  2.5195043087005615\n",
      "are | different, or, usually, can, have\n",
      "some | many, it, not, to, such\n",
      "first | the, in, for, second, one\n",
      "by | and, in, while, the, of\n",
      "these | generally, are, or, many, from\n",
      "an | in, the, for, a, of\n",
      "its | from, the, has, is, which\n",
      "it | be, not, to, is, a\n",
      "know | say, you, knows, cannot, saying\n",
      "pressure | heat, liquid, resonance, pressures, energy\n",
      "orthodox | christianity, orthodoxy, churches, christians, catholic\n",
      "troops | army, forces, deployed, war, allied\n",
      "consists | consisting, separate, adjacent, respectively, or\n",
      "hit | hits, hitting, scored, ball, batter\n",
      "engineering | institute, technology, engineers, research, science\n",
      "professional | amateur, games, sports, management, soccer\n",
      "...\n",
      "\n",
      "Epoch: 5/25\n",
      "Loss:  2.4175446033477783\n",
      "united | states, canada, member, government, national\n",
      "american | born, politician, seven, singer, musician\n",
      "used | uses, for, use, devices, as\n",
      "often | sometimes, or, many, are, usually\n",
      "all | are, to, this, is, not\n",
      "however | it, to, have, and, as\n",
      "b | p, d, h, k, politician\n",
      "this | is, any, the, all, that\n",
      "operations | operation, tasks, logistics, program, real\n",
      "award | awards, nominated, grammy, awarded, academy\n",
      "construction | buildings, constructed, building, reconstruction, highway\n",
      "shown | these, be, called, and, so\n",
      "rise | significant, increased, increasing, effect, due\n",
      "active | plasma, conferences, pinewood, non, activities\n",
      "liberal | liberals, party, democratic, conservative, democrats\n",
      "creation | creationism, development, created, judaism, narratives\n",
      "...\n",
      "\n",
      "Epoch: 5/25\n",
      "Loss:  2.181523561477661\n",
      "or | is, are, either, be, may\n",
      "there | that, some, of, have, are\n",
      "to | of, not, the, that, a\n",
      "for | of, also, to, and, the\n",
      "at | three, seven, one, after, four\n",
      "may | or, are, that, others, is\n",
      "zero | two, five, six, seven, three\n",
      "he | his, him, himself, was, after\n",
      "ocean | pacific, atlantic, island, eutelsat, antarctic\n",
      "smith | robert, john, jackson, michael, stewart\n",
      "primarily | largely, generally, into, century, areas\n",
      "animals | animal, humans, insects, predators, species\n",
      "applied | used, term, practical, properties, definition\n",
      "consists | divisions, divided, consist, consisting, multi\n",
      "orthodox | churches, church, christianity, christians, constantinople\n",
      "defense | defence, commission, military, security, deployed\n",
      "...\n",
      "\n",
      "Epoch: 5/25\n",
      "Loss:  2.3027875423431396\n",
      "s | the, in, one, his, of\n",
      "see | list, references, links, external, of\n",
      "people | who, americans, believe, living, to\n",
      "states | united, state, u, countries, nations\n",
      "often | can, other, a, that, sometimes\n",
      "one | eight, seven, nine, five, six\n",
      "history | links, references, see, timeline, website\n",
      "than | more, only, less, high, much\n",
      "active | post, party, present, the, groups\n",
      "scale | ratios, scales, range, large, c\n",
      "bill | george, bob, canadian, senator, evans\n",
      "event | events, extinction, celebration, when, decision\n",
      "shown | discussed, transcription, characterization, symptoms, never\n",
      "animals | humans, animal, insects, mammals, species\n",
      "paris | louvre, france, pierre, de, le\n",
      "governor | president, appointed, lieutenant, state, queen\n",
      "...\n",
      "\n",
      "Epoch: 5/25\n",
      "Loss:  2.3871238231658936\n",
      "however | be, to, has, although, other\n",
      "some | are, other, more, many, for\n",
      "or | can, usually, is, and, either\n",
      "i | t, n, j, then, let\n",
      "be | to, are, is, for, can\n",
      "two | zero, five, three, four, one\n",
      "such | other, and, as, for, with\n",
      "after | was, he, his, the, had\n",
      "existence | meaning, ontological, deities, within, supernatural\n",
      "prince | crown, son, princess, queen, throne\n",
      "engine | engines, combustion, fuel, diesel, powered\n",
      "paris | france, le, louvre, sur, de\n",
      "discovered | discovery, discoveries, discoverer, identified, specimens\n",
      "governor | president, lieutenant, appointed, governors, chief\n",
      "accepted | accept, decision, rejected, refused, claim\n",
      "gold | silver, coins, tin, zinc, iron\n",
      "...\n",
      "\n",
      "Epoch: 6/25\n",
      "Loss:  2.3833909034729004\n",
      "while | up, the, for, and, when\n",
      "he | his, him, had, himself, was\n",
      "a | the, to, is, of, and\n",
      "is | and, a, the, of, as\n",
      "b | d, politician, composer, laureate, writer\n",
      "however | but, not, to, from, was\n",
      "known | in, the, are, as, well\n",
      "state | government, national, legislature, district, of\n",
      "account | accounts, can, origin, present, evidence\n",
      "smith | harris, joseph, bryan, jones, robert\n",
      "http | www, com, html, org, htm\n",
      "derived | name, originally, latin, etymology, derivation\n",
      "construction | building, manufacturing, textiles, built, buildings\n",
      "orthodox | christians, church, christianity, orthodoxy, eastern\n",
      "professional | football, leagues, sports, professionals, teams\n",
      "assembly | elections, parliament, legislative, vote, parliamentary\n",
      "...\n",
      "\n",
      "Epoch: 6/25\n",
      "Loss:  2.276409387588501\n",
      "on | by, and, in, was, of\n",
      "american | african, canadian, united, born, americans\n",
      "known | as, in, most, the, from\n",
      "where | is, when, delta, the, an\n",
      "all | are, as, and, not, for\n",
      "use | used, common, like, or, as\n",
      "have | are, to, as, that, although\n",
      "which | the, with, to, a, this\n",
      "test | tests, testing, cricket, cricketers, ashes\n",
      "active | activities, on, present, policy, ekwele\n",
      "operating | unix, hardware, bsd, system, processor\n",
      "discovered | discovery, discoverer, astronomer, discoveries, earth\n",
      "older | median, families, females, years, household\n",
      "engine | engines, powered, cylinder, jet, turbine\n",
      "consists | consisting, small, consist, upper, adjacent\n",
      "defense | defensive, civilian, defending, force, defence\n",
      "...\n",
      "\n",
      "Epoch: 6/25\n",
      "Loss:  2.1167593002319336\n",
      "can | be, properties, are, is, used\n",
      "is | the, of, an, a, are\n",
      "there | are, that, have, only, be\n",
      "between | both, has, and, the, of\n",
      "when | to, be, with, a, so\n",
      "than | less, more, much, very, often\n",
      "system | systems, allows, operating, used, useful\n",
      "its | it, which, an, the, has\n",
      "resources | resource, arable, agricultural, economic, petroleum\n",
      "magazine | published, magazines, comic, journal, comics\n",
      "shows | show, tv, performing, on, an\n",
      "egypt | egyptian, cairo, arab, syria, palestinian\n",
      "existence | notion, itself, that, universe, our\n",
      "joseph | smith, george, maurice, eight, robert\n",
      "derived | or, etymology, form, name, referred\n",
      "discovered | discovery, discoveries, discoverer, planets, detected\n",
      "...\n",
      "\n",
      "Epoch: 6/25\n",
      "Loss:  2.053088665008545\n",
      "would | be, not, could, it, have\n",
      "who | him, whom, to, them, himself\n",
      "four | three, one, two, zero, five\n",
      "also | is, of, and, as, the\n",
      "three | four, two, one, zero, six\n",
      "has | is, however, of, also, as\n",
      "such | used, are, types, have, as\n",
      "zero | two, five, four, three, one\n",
      "channel | channels, stations, cable, tv, radio\n",
      "scale | mass, scales, approximately, earth, increasing\n",
      "operations | operation, aircraft, logistics, combat, unit\n",
      "mean | word, geometric, means, defined, approximate\n",
      "know | you, i, we, why, really\n",
      "police | military, officers, civilian, authorities, officials\n",
      "existence | universe, concept, god, itself, afterlife\n",
      "account | about, around, accuracy, estimates, accounts\n",
      "...\n",
      "\n",
      "Epoch: 6/25\n",
      "Loss:  2.353299140930176\n",
      "more | than, has, in, very, as\n",
      "who | whom, to, was, he, by\n",
      "called | a, or, as, refers, such\n",
      "and | in, of, as, the, by\n",
      "first | the, in, was, a, later\n",
      "their | they, not, of, to, only\n",
      "his | him, himself, he, had, was\n",
      "new | york, became, also, first, the\n",
      "consists | consist, consisting, each, divided, comprises\n",
      "governor | president, appointed, governors, lieutenant, elected\n",
      "grand | prix, royal, district, six, lodge\n",
      "san | francisco, diego, los, jose, california\n",
      "gold | silver, ore, copper, precious, coins\n",
      "stage | actors, theatre, roles, movie, portrayed\n",
      "shows | television, show, tv, comedy, scenes\n",
      "mean | always, word, given, infty, any\n",
      "...\n",
      "\n",
      "Epoch: 6/25\n",
      "Loss:  2.206644058227539\n",
      "eight | one, seven, six, four, five\n",
      "th | century, rd, centuries, nd, gregorian\n",
      "after | was, before, the, in, became\n",
      "at | in, the, of, and, first\n",
      "their | they, have, with, the, had\n",
      "than | less, be, can, it, more\n",
      "see | list, links, external, article, of\n",
      "between | is, the, line, are, along\n",
      "grand | prix, duke, titles, hrigen, district\n",
      "hold | valid, will, those, not, themselves\n",
      "award | awards, academy, best, nominated, nominations\n",
      "scale | measuring, scales, tuning, richter, harmonic\n",
      "bbc | listing, day, news, march, april\n",
      "shows | show, tv, television, films, episode\n",
      "troops | army, forces, war, brigade, evacuated\n",
      "professional | sports, professionals, amateur, football, games\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/25\n",
      "Loss:  2.124408483505249\n",
      "all | are, have, only, some, and\n",
      "or | are, as, often, typically, be\n",
      "after | was, in, at, by, nine\n",
      "i | want, me, you, t, we\n",
      "that | is, be, not, to, have\n",
      "their | they, have, but, and, were\n",
      "as | the, of, and, to, or\n",
      "however | but, not, have, that, some\n",
      "stage | film, phase, having, minutes, theatre\n",
      "centre | metropolitan, located, town, centres, situated\n",
      "active | passive, activities, other, low, helping\n",
      "road | roads, railway, routes, park, highway\n",
      "running | run, slower, line, backs, ran\n",
      "versions | windows, version, features, configuration, software\n",
      "proposed | proposals, debate, link, propose, established\n",
      "lived | half, father, died, was, grandfather\n",
      "...\n",
      "\n",
      "Epoch: 7/25\n",
      "Loss:  2.2744295597076416\n",
      "up | the, in, a, by, while\n",
      "and | in, as, a, the, of\n",
      "be | can, that, might, to, is\n",
      "called | when, is, as, a, the\n",
      "the | of, in, a, for, as\n",
      "been | by, has, was, in, had\n",
      "but | this, with, to, from, as\n",
      "system | systems, operating, which, used, of\n",
      "institute | university, college, technology, school, engineering\n",
      "freedom | freedoms, liberty, economic, governments, liberties\n",
      "taking | their, involving, setting, to, which\n",
      "lived | had, died, his, occupied, who\n",
      "quite | very, while, some, different, more\n",
      "brother | wife, son, she, father, married\n",
      "issue | policies, documents, publication, publicized, rehnquist\n",
      "mathematics | mathematical, algebraic, geometry, theorems, algebra\n",
      "...\n",
      "\n",
      "Epoch: 7/25\n",
      "Loss:  2.2854628562927246\n",
      "a | in, the, an, with, of\n",
      "up | when, out, a, each, the\n",
      "in | the, of, a, by, s\n",
      "an | a, for, the, is, which\n",
      "th | century, three, two, zero, six\n",
      "is | of, the, a, be, or\n",
      "his | he, s, himself, him, was\n",
      "who | he, whom, him, to, child\n",
      "brother | son, sons, wife, married, father\n",
      "joseph | john, smith, jr, cowdery, mormon\n",
      "hold | held, holds, put, will, then\n",
      "question | questions, knowledge, truth, contrary, whether\n",
      "quite | less, very, aware, have, can\n",
      "ice | frozen, rocks, surface, melted, arctic\n",
      "centre | around, dublin, railway, london, regional\n",
      "articles | org, page, encyclopedia, links, list\n",
      "...\n",
      "\n",
      "Epoch: 7/25\n",
      "Loss:  2.136115074157715\n",
      "had | was, later, were, eventually, his\n",
      "and | of, the, from, most, as\n",
      "use | or, such, may, often, used\n",
      "these | are, some, to, however, and\n",
      "i | me, you, t, am, my\n",
      "up | that, the, before, a, off\n",
      "known | is, as, first, most, of\n",
      "over | s, all, only, years, under\n",
      "nobel | laureate, prize, physicist, physiology, chemist\n",
      "operations | operation, numbers, multiplication, operational, assets\n",
      "applied | term, refers, characteristic, describe, usually\n",
      "governor | governors, appointed, cabinet, president, chief\n",
      "powers | constitution, constitutional, government, parliamentarians, chief\n",
      "existence | notion, universe, idealism, god, argued\n",
      "centre | founded, located, around, city, tourist\n",
      "units | unit, infantry, were, battalions, concentration\n",
      "...\n",
      "\n",
      "Epoch: 7/25\n",
      "Loss:  2.228576898574829\n",
      "by | as, in, and, the, of\n",
      "during | of, on, early, war, period\n",
      "which | of, in, the, for, a\n",
      "from | the, to, and, of, a\n",
      "had | was, were, his, began, later\n",
      "is | also, or, a, for, which\n",
      "would | to, when, not, could, was\n",
      "a | the, for, to, as, on\n",
      "running | run, ran, along, while, road\n",
      "report | reported, reports, assessment, ipcc, fbi\n",
      "ice | skating, hockey, frozen, glaciers, melted\n",
      "file | files, executable, sharing, unix, xml\n",
      "alternative | label, commonly, major, classical, minor\n",
      "scale | scales, ratios, range, chromatic, oscillation\n",
      "universe | universes, cosmological, cosmology, cosmic, comics\n",
      "governor | appointed, president, governors, cabinet, department\n",
      "...\n",
      "\n",
      "Epoch: 7/25\n",
      "Loss:  1.961199164390564\n",
      "would | but, to, could, did, because\n",
      "one | nine, seven, eight, four, six\n",
      "new | york, the, first, in, zero\n",
      "which | the, a, was, as, this\n",
      "nine | one, seven, eight, four, three\n",
      "up | a, into, very, then, around\n",
      "system | systems, operating, based, code, software\n",
      "he | his, himself, after, him, was\n",
      "troops | army, forces, surrender, war, soldiers\n",
      "egypt | egyptian, sinai, cairo, alexandria, nile\n",
      "numerous | many, most, notably, including, particularly\n",
      "experience | experiences, creative, mind, physical, feeling\n",
      "notes | chord, octave, tuned, impressed, strings\n",
      "proposed | proposals, principle, sharing, adopted, discovery\n",
      "pope | papal, archbishop, gregory, eugenius, vii\n",
      "http | www, html, htm, org, edu\n",
      "...\n",
      "\n",
      "Epoch: 8/25\n",
      "Loss:  2.08768630027771\n",
      "people | americans, their, community, themselves, children\n",
      "one | five, eight, six, seven, two\n",
      "for | and, to, the, more, on\n",
      "known | in, of, with, and, also\n",
      "however | but, not, to, more, some\n",
      "such | as, some, including, many, they\n",
      "the | in, of, and, a, is\n",
      "this | to, be, is, has, have\n",
      "issue | issues, arbitrator, documents, opposition, decision\n",
      "account | narrative, biblical, accounts, sources, antiquities\n",
      "quite | but, very, often, to, it\n",
      "animals | animal, humans, beings, eating, human\n",
      "applied | practice, study, processes, properties, materials\n",
      "gold | silver, ore, precious, coins, copper\n",
      "http | www, html, htm, org, com\n",
      "know | you, we, i, want, everything\n",
      "...\n",
      "\n",
      "Epoch: 8/25\n",
      "Loss:  2.1272647380828857\n",
      "there | many, are, all, and, is\n",
      "be | is, can, not, that, a\n",
      "state | university, county, college, institute, governors\n",
      "their | they, to, own, and, that\n",
      "its | has, which, from, is, this\n",
      "will | you, to, must, next, not\n",
      "for | of, the, a, to, is\n",
      "which | the, as, from, to, is\n",
      "freedom | liberty, equality, egoism, politicians, willing\n",
      "except | to, only, some, from, long\n",
      "resources | resource, mining, information, online, ore\n",
      "magazine | interview, forums, magazines, editor, published\n",
      "police | military, prosecution, officers, officials, paramilitary\n",
      "orthodox | orthodoxy, christians, liturgical, oriental, christianity\n",
      "something | nothing, you, else, thing, what\n",
      "square | squares, kilometers, sq, km, cubic\n",
      "...\n",
      "\n",
      "Epoch: 8/25\n",
      "Loss:  2.300178289413452\n",
      "after | him, had, was, before, first\n",
      "it | thus, that, to, be, this\n",
      "however | the, has, as, that, this\n",
      "by | the, s, and, to, been\n",
      "would | to, did, could, it, they\n",
      "eight | one, seven, nine, six, five\n",
      "a | an, in, the, as, to\n",
      "also | the, as, are, other, has\n",
      "dr | harvey, doctor, villain, episode, animated\n",
      "applications | computing, application, technologies, systems, components\n",
      "powers | constitution, constitutional, bicameral, government, dominion\n",
      "grand | knights, prix, st, champion, battle\n",
      "account | sources, relates, given, been, work\n",
      "joseph | john, smith, robert, benjamin, chemist\n",
      "proposed | propose, postulated, been, debate, proposals\n",
      "behind | off, left, caught, down, onto\n",
      "...\n",
      "\n",
      "Epoch: 8/25\n",
      "Loss:  2.3454065322875977\n",
      "this | that, the, it, of, in\n",
      "four | one, five, two, three, six\n",
      "would | to, be, when, it, will\n",
      "their | they, to, were, who, the\n",
      "most | and, these, are, of, considered\n",
      "the | in, of, by, and, was\n",
      "during | were, was, beginning, the, period\n",
      "first | was, second, of, last, a\n",
      "grand | prix, knights, painted, eight, champion\n",
      "animals | animal, mammals, eating, species, humans\n",
      "frac | cdot, int, cos, equation, pi\n",
      "recorded | records, recording, songs, recordings, album\n",
      "smith | michael, harris, bryan, joseph, brian\n",
      "troops | army, invasion, soldiers, armies, forces\n",
      "magazine | interview, magazines, news, fan, editor\n",
      "engine | engines, powered, combustion, cylinder, turbo\n",
      "...\n",
      "\n",
      "Epoch: 8/25\n",
      "Loss:  2.320387840270996\n",
      "state | states, united, districts, the, representatives\n",
      "there | some, are, or, only, is\n",
      "one | six, four, nine, eight, zero\n",
      "while | their, by, that, his, it\n",
      "after | was, in, he, at, before\n",
      "would | when, to, was, did, after\n",
      "it | that, to, be, the, this\n",
      "often | other, is, sometimes, a, be\n",
      "creation | conceived, envisioned, creating, creator, into\n",
      "applied | term, ethics, adhere, berenyi, fyrd\n",
      "ice | hockey, skating, glaciers, sheets, rocks\n",
      "road | roads, highway, rail, railway, tram\n",
      "defense | defence, defensive, staff, military, insanity\n",
      "pressure | pressures, temperature, resistance, violently, resulting\n",
      "proposed | adoption, strathaven, debate, measurements, propose\n",
      "paris | beaux, lyon, toulouse, brussels, le\n",
      "...\n",
      "\n",
      "Epoch: 8/25\n",
      "Loss:  2.299396276473999\n",
      "state | county, counties, states, republican, southern\n",
      "by | an, s, of, the, to\n",
      "if | then, be, we, case, suppose\n",
      "and | in, of, the, for, is\n",
      "after | was, in, became, he, before\n",
      "seven | eight, one, nine, four, six\n",
      "the | in, a, of, to, and\n",
      "known | is, of, first, as, a\n",
      "pressure | temperature, cooling, liquid, compressor, diaphragm\n",
      "mainly | important, many, mostly, especially, among\n",
      "http | www, htm, org, html, com\n",
      "hold | make, will, way, accordingly, those\n",
      "test | tests, testing, tested, program, flight\n",
      "bbc | listing, day, news, radio, itv\n",
      "frac | cdot, cos, sum, equation, omega\n",
      "construction | building, completed, bridge, concrete, constructed\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/25\n",
      "Loss:  2.0929183959960938\n",
      "used | use, uses, describe, commonly, similar\n",
      "which | a, for, called, an, the\n",
      "eight | one, six, four, two, three\n",
      "more | than, most, is, these, and\n",
      "if | any, be, must, whether, it\n",
      "years | last, period, year, after, remaining\n",
      "see | also, links, list, references, external\n",
      "are | or, is, different, these, be\n",
      "alternative | complementary, medicine, list, advocates, methods\n",
      "brother | son, sons, his, throne, daughter\n",
      "something | know, somehow, idea, say, useless\n",
      "nobel | prize, laureate, physicist, recipient, chemist\n",
      "taking | up, in, he, proceed, control\n",
      "centre | located, largest, shopping, situated, city\n",
      "applied | ethics, also, classified, is, type\n",
      "placed | wooden, were, once, belonging, constructed\n",
      "...\n",
      "\n",
      "Epoch: 9/25\n",
      "Loss:  2.2824854850769043\n",
      "between | from, are, separated, and, within\n",
      "than | more, are, is, different, to\n",
      "history | references, links, historical, external, modern\n",
      "five | three, one, four, zero, two\n",
      "during | in, was, period, lasted, after\n",
      "has | of, is, been, in, a\n",
      "a | in, the, is, that, and\n",
      "however | have, that, in, it, the\n",
      "creation | existence, created, god, creationism, faith\n",
      "construction | completed, building, built, tower, buildings\n",
      "engine | engines, turbine, car, powered, jet\n",
      "magazine | published, interview, monthly, external, editor\n",
      "applied | is, fundamental, describe, application, physics\n",
      "question | questions, what, whether, we, argument\n",
      "mean | word, average, variance, is, limit\n",
      "mainly | primarily, predominantly, most, mostly, diverse\n",
      "...\n",
      "\n",
      "Epoch: 9/25\n",
      "Loss:  2.261981725692749\n",
      "seven | one, six, five, four, three\n",
      "their | many, some, they, and, were\n",
      "if | can, then, be, we, i\n",
      "after | was, him, went, last, the\n",
      "united | states, canada, commission, union, u\n",
      "from | in, of, the, to, and\n",
      "as | the, of, to, or, for\n",
      "his | he, s, him, friend, himself\n",
      "file | files, format, xml, user, bytes\n",
      "derived | related, origin, or, etymology, term\n",
      "know | you, we, say, i, your\n",
      "lived | was, were, and, became, he\n",
      "behind | scenes, left, off, front, opening\n",
      "versions | version, windows, included, microsoft, original\n",
      "active | other, activities, site, years, cyclase\n",
      "shows | show, shown, tv, episode, showing\n",
      "...\n",
      "\n",
      "Epoch: 9/25\n",
      "Loss:  2.187973976135254\n",
      "states | united, state, u, congress, during\n",
      "one | nine, seven, six, four, five\n",
      "system | systems, operating, software, computers, installed\n",
      "state | legislature, government, states, national, governors\n",
      "some | have, be, many, are, that\n",
      "with | to, the, s, and, of\n",
      "four | one, two, six, zero, three\n",
      "called | also, the, of, are, or\n",
      "hold | not, but, that, be, could\n",
      "brother | son, wife, throne, father, king\n",
      "experience | our, consciousness, experiences, thinking, subjective\n",
      "bill | bills, amend, clinton, president, rights\n",
      "accepted | that, proposed, scholars, universally, evidence\n",
      "award | awards, awarded, won, best, nominated\n",
      "writers | novelists, poets, anthology, births, poetry\n",
      "except | additional, because, than, into, only\n",
      "...\n",
      "\n",
      "Epoch: 9/25\n",
      "Loss:  2.288778305053711\n",
      "american | actor, nine, writer, singer, activist\n",
      "one | seven, six, nine, five, eight\n",
      "about | and, how, of, a, an\n",
      "with | the, in, a, and, to\n",
      "during | was, early, of, his, had\n",
      "some | are, other, there, many, that\n",
      "be | not, it, can, to, that\n",
      "if | not, be, it, must, can\n",
      "arts | martial, masters, college, education, institute\n",
      "creation | into, campaign, created, confrontation, attempting\n",
      "square | located, miles, kilometers, area, cubic\n",
      "bill | bills, clinton, november, presidential, united\n",
      "frac | cdot, cos, equation, sum, mathrm\n",
      "channel | channels, cable, irc, networks, signal\n",
      "proposed | adoption, accepted, propose, discussed, proposals\n",
      "experience | knowledge, mental, subjective, mind, consciousness\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Get our noise distribution\n",
    "# Using word frequencies calculated earlier in the notebook\n",
    "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
    "unigram_dist = word_freqs/word_freqs.sum()\n",
    "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
    "\n",
    "# instantiating the model\n",
    "embedding_dim = 300\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
    "\n",
    "# using the loss that we defined\n",
    "criterion = NegativeSamplingLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "print_every = 1500\n",
    "steps = 0\n",
    "epochs = 25\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # get our input, target batches\n",
    "    for input_words, target_words in get_batches(train_words, 512):\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # input, outpt, and noise vectors\n",
    "        input_vectors = model.forward_input(inputs)\n",
    "        output_vectors = model.forward_output(targets)\n",
    "        noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
    "\n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
    "            print(\"Loss: \", loss.item()) # avg batch loss at this point in training\n",
    "            valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6)\n",
    "\n",
    "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
    "            print(\"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the word vectors\n",
    "\n",
    "Below we'll use T-SNE to visualize how our high-dimensional word vectors cluster together. T-SNE is used to project these vectors into two dimensions while preserving local stucture. Check out [this post from Christopher Olah](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) to learn more about T-SNE and other ways to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-32281b40fdd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# getting embeddings from the embedding layer of our model, by name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.in_embed.weight.to('cpu').data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0000d57a567f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mviz_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m380\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membed_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mviz_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "viz_words = 380\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_tsne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c5609e682f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviz_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0membed_tsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steelblue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_to_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membed_tsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_tsne\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embed_tsne' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 904,
       "width": 938
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
