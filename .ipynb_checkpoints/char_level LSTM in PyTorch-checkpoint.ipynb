{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level LSTM in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Statistical Language Model__: A trained model to predict the next word/character given all previous words/characters.\n",
    "\n",
    "__Character-Level Language Model__: The main task of the char-level language model is to predict the next character given all previous characters in a sequence of data, i.e. generates text character by character. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on an intrigue with a French\\ngirl, who had been a governess in their family, and she had announced to\\nher husband that she could not go on living in the same house with him.\\nThis position of affairs had now lasted three days, and not only the\\nhusband and wife themselves, but all the members of their family and\\nhousehold, were painfully conscious of it. Every person in the house\\nfelt that there was no sense in their living together, and that the\\nstray people brought together by chance in any inn had more in common\\nwith one another than they, the members of the family and household of\\nthe Oblonskys. The wife did not leave her own room, the husband had not\\nbeen at home for three days. The children ran wild all over the house;\\nthe English governess quarreled with the housekeeper, and wrote to a\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding the text ## \n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:ii for ii,ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 32, 79, 26, 46, 52, 82, 20, 37, 27, 27, 27, 47, 79, 26, 26, 74,\n",
       "       20, 71, 79, 53, 45, 80, 45, 52, 62, 20, 79, 82, 52, 20, 79, 80, 80,\n",
       "       20, 79, 80, 45, 49, 52,  2, 20, 52, 72, 52, 82, 74, 20, 19, 64, 32,\n",
       "       79, 26, 26, 74, 20, 71, 79, 53, 45, 80, 74, 20, 45, 62, 20, 19, 64,\n",
       "       32, 79, 26, 26, 74, 20, 45, 64, 20, 45, 46, 62, 20, 40, 75, 64, 27,\n",
       "       75, 79, 74,  7, 27, 27, 10, 72, 52, 82, 74, 46, 32, 45, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    #n_labels = max(arr.flatten()) + 1\n",
    "    \n",
    "    one_hot = np.zeros(shape = (np.multiply(*arr.shape) , n_labels))\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = np.array([[1,2,3,7],[5,3,2,8]])\n",
    "one_hot = one_hot_encode(test_seq,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N: batch size\n",
    "# M: sequence length\n",
    "# K: total number of batches\n",
    "\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    \n",
    "    # Number of matches we can make from the input array\n",
    "    n_batches = len(arr) // (batch_size * seq_length)\n",
    "    \n",
    "    # keeping enoough character to make full batches\n",
    "    arr = arr[:n_batches * batch_size * seq_length]\n",
    "    \n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterating over the batches\n",
    "    for n in range(0, arr.shape[1] , seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1], y[:,-1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1], y[:,-1] = x[:, 1:], arr[:,0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the get_batch function\n",
    "\n",
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[77, 32, 79, 26, 46, 52, 82, 20, 37, 27, 27, 27, 47, 79, 26, 26,\n",
       "         74, 20, 71, 79, 53, 45, 80, 45, 52, 62, 20, 79, 82, 52, 20, 79,\n",
       "         80, 80, 20, 79, 80, 45, 49, 52,  2, 20, 52, 72, 52, 82, 74, 20,\n",
       "         19, 64],\n",
       "        [62, 40, 64, 20, 46, 32, 79, 46, 20, 79, 46, 46, 82, 79,  5, 46,\n",
       "         52, 17, 20, 32, 52, 82, 20, 79, 46, 46, 52, 64, 46, 45, 40, 64,\n",
       "         20, 75, 79, 62, 20, 32, 52, 82, 20, 32, 19, 62, 68, 79, 64, 17,\n",
       "          7, 20],\n",
       "        [52, 64, 17, 20, 40, 82, 20, 79, 20, 71, 40, 52, 28, 20, 32, 52,\n",
       "         20, 79, 72, 40, 45, 17, 52, 17, 20, 32, 45, 62, 20, 71, 79, 46,\n",
       "         32, 52, 82,  7, 20, 47, 52, 27, 80, 40, 40, 49, 52, 17, 20, 82,\n",
       "         40, 19],\n",
       "        [62, 20, 46, 32, 52, 20,  5, 32, 45, 52, 71, 20, 46, 32, 40, 19,\n",
       "         60, 32, 20, 32, 45, 17, 17, 52, 64, 27, 45, 64, 46, 52, 82, 52,\n",
       "         62, 46, 20, 40, 71, 20, 32, 45, 62, 20, 80, 45, 71, 52, 28, 20,\n",
       "         40, 71],\n",
       "        [20, 62, 79, 75, 20, 32, 52, 82, 20, 46, 52, 79, 82, 16, 62, 46,\n",
       "         79, 45, 64, 52, 17, 28, 20, 26, 45, 46, 45, 71, 19, 80, 28, 20,\n",
       "         62, 75, 52, 52, 46, 20, 71, 79,  5, 52, 28, 27, 53, 45, 62, 52,\n",
       "         82, 79],\n",
       "        [ 5, 19, 62, 62, 45, 40, 64, 20, 79, 64, 17, 20, 79, 64, 79, 80,\n",
       "         74, 62, 45, 62, 28, 20, 75, 79, 62, 20, 45, 64, 20, 26, 82, 45,\n",
       "         64,  5, 45, 26, 80, 52, 20, 17, 45, 62, 79, 60, 82, 52, 52, 79,\n",
       "         68, 80],\n",
       "        [20, 65, 64, 64, 79, 20, 32, 79, 17, 20, 62, 79, 45, 17, 20, 46,\n",
       "         32, 79, 46, 20, 41, 40, 80, 80, 74, 20, 75, 40, 19, 80, 17, 20,\n",
       "         52, 51,  5, 19, 62, 52, 20, 45, 46,  7, 20, 65, 64, 17, 20, 46,\n",
       "         32, 45],\n",
       "        [24, 68, 80, 40, 64, 62, 49, 74,  7, 20, 43, 50, 19, 46, 20, 22,\n",
       "         46, 32, 52, 74, 22, 20,  5, 79, 64, 64, 40, 46, 20, 60, 82, 79,\n",
       "         62, 26, 20, 46, 32, 79, 46, 28, 27, 22, 46, 32, 52, 74, 22, 20,\n",
       "         79, 82]]),\n",
       " array([[32, 79, 26, 46, 52, 82, 20, 37, 27, 27, 27, 47, 79, 26, 26, 74,\n",
       "         20, 71, 79, 53, 45, 80, 45, 52, 62, 20, 79, 82, 52, 20, 79, 80,\n",
       "         80, 20, 79, 80, 45, 49, 52,  2, 20, 52, 72, 52, 82, 74, 20, 19,\n",
       "         64, 32],\n",
       "        [40, 64, 20, 46, 32, 79, 46, 20, 79, 46, 46, 82, 79,  5, 46, 52,\n",
       "         17, 20, 32, 52, 82, 20, 79, 46, 46, 52, 64, 46, 45, 40, 64, 20,\n",
       "         75, 79, 62, 20, 32, 52, 82, 20, 32, 19, 62, 68, 79, 64, 17,  7,\n",
       "         20, 43],\n",
       "        [64, 17, 20, 40, 82, 20, 79, 20, 71, 40, 52, 28, 20, 32, 52, 20,\n",
       "         79, 72, 40, 45, 17, 52, 17, 20, 32, 45, 62, 20, 71, 79, 46, 32,\n",
       "         52, 82,  7, 20, 47, 52, 27, 80, 40, 40, 49, 52, 17, 20, 82, 40,\n",
       "         19, 64],\n",
       "        [20, 46, 32, 52, 20,  5, 32, 45, 52, 71, 20, 46, 32, 40, 19, 60,\n",
       "         32, 20, 32, 45, 17, 17, 52, 64, 27, 45, 64, 46, 52, 82, 52, 62,\n",
       "         46, 20, 40, 71, 20, 32, 45, 62, 20, 80, 45, 71, 52, 28, 20, 40,\n",
       "         71, 20],\n",
       "        [62, 79, 75, 20, 32, 52, 82, 20, 46, 52, 79, 82, 16, 62, 46, 79,\n",
       "         45, 64, 52, 17, 28, 20, 26, 45, 46, 45, 71, 19, 80, 28, 20, 62,\n",
       "         75, 52, 52, 46, 20, 71, 79,  5, 52, 28, 27, 53, 45, 62, 52, 82,\n",
       "         79, 68],\n",
       "        [19, 62, 62, 45, 40, 64, 20, 79, 64, 17, 20, 79, 64, 79, 80, 74,\n",
       "         62, 45, 62, 28, 20, 75, 79, 62, 20, 45, 64, 20, 26, 82, 45, 64,\n",
       "          5, 45, 26, 80, 52, 20, 17, 45, 62, 79, 60, 82, 52, 52, 79, 68,\n",
       "         80, 52],\n",
       "        [65, 64, 64, 79, 20, 32, 79, 17, 20, 62, 79, 45, 17, 20, 46, 32,\n",
       "         79, 46, 20, 41, 40, 80, 80, 74, 20, 75, 40, 19, 80, 17, 20, 52,\n",
       "         51,  5, 19, 62, 52, 20, 45, 46,  7, 20, 65, 64, 17, 20, 46, 32,\n",
       "         45, 62],\n",
       "        [68, 80, 40, 64, 62, 49, 74,  7, 20, 43, 50, 19, 46, 20, 22, 46,\n",
       "         32, 52, 74, 22, 20,  5, 79, 64, 64, 40, 46, 20, 60, 82, 79, 62,\n",
       "         26, 20, 46, 32, 79, 46, 28, 27, 22, 46, 32, 52, 74, 22, 20, 79,\n",
       "         82, 52]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, chars, n_hidden = 256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.chars = chars\n",
    "        \n",
    "        self.int2chars = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch : ii for ii,ch in self.int2chars.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = len(self.chars), hidden_size = n_hidden,  num_layers = n_layers, \\\n",
    "                             dropout = self.drop_prob, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(self.n_hidden, len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # creating two new tensors with size n_layers * batch_size * n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of the LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "            \n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, \\\n",
    "          val_frac=0.1, print_every=10, epoch_offset = 0,save_path = \"model.pt\"):\n",
    "    '''\n",
    "        Arguments:\n",
    "            net: CharRNN network\n",
    "            data: text data to train the network\n",
    "            epochs: number of epochs\n",
    "            batch_size: number of mini-sequences per mini-batch\n",
    "            seq_length: Number of character steps per mini-batch\n",
    "            lr: learning rate\n",
    "            clip: gradient clipping\n",
    "            val_frac: Fraction of data to hold out for validation\n",
    "            print_every: number of steps for printing training and validation\n",
    "    '''\n",
    "    \n",
    "    # Making all parameters of the model Double\n",
    "    net.double()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # creating training and validation data\n",
    "    val_idx = int(len(data) * (1 - val_frac))\n",
    "    data , val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epoch_offset,  epoch_offset + epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                inputs , targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            #creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses=[]\n",
    "                net.eval()\n",
    "                for x,y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x,y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x,y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                    \n",
    "                    val_losses.append(val_loss.item())\n",
    "            \n",
    "                net.train()\n",
    "                \n",
    "                new_val_loss = np.mean(val_losses)\n",
    "                \n",
    "                if new_val_loss < valid_loss_min:\n",
    "                    valid_loss_min = new_val_loss\n",
    "                    torch.save(net.state_dict(), save_path)\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.4f}\".format(new_val_loss))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2934... Val Loss: 3.2490\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1470... Val Loss: 3.1393\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1443... Val Loss: 3.1257\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1156... Val Loss: 3.1191\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1471... Val Loss: 3.1178\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1196... Val Loss: 3.1162\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1104... Val Loss: 3.1154\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1254... Val Loss: 3.1130\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1240... Val Loss: 3.1071\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1062... Val Loss: 3.0989\n",
      "Epoch: 1/20... Step: 110... Loss: 3.0865... Val Loss: 3.0667\n",
      "Epoch: 1/20... Step: 120... Loss: 2.9947... Val Loss: 2.9856\n",
      "Epoch: 1/20... Step: 130... Loss: 2.9743... Val Loss: 2.9341\n",
      "Epoch: 2/20... Step: 140... Loss: 2.8703... Val Loss: 2.8226\n",
      "Epoch: 2/20... Step: 150... Loss: 2.7537... Val Loss: 2.7147\n",
      "Epoch: 2/20... Step: 160... Loss: 2.6497... Val Loss: 2.6166\n",
      "Epoch: 2/20... Step: 170... Loss: 2.5541... Val Loss: 2.5426\n",
      "Epoch: 2/20... Step: 180... Loss: 2.5209... Val Loss: 2.4935\n",
      "Epoch: 2/20... Step: 190... Loss: 2.4676... Val Loss: 2.4609\n",
      "Epoch: 2/20... Step: 200... Loss: 2.4552... Val Loss: 2.4194\n",
      "Epoch: 2/20... Step: 210... Loss: 2.4140... Val Loss: 2.3955\n",
      "Epoch: 2/20... Step: 220... Loss: 2.3850... Val Loss: 2.3620\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3672... Val Loss: 2.3350\n",
      "Epoch: 2/20... Step: 240... Loss: 2.3486... Val Loss: 2.3141\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2884... Val Loss: 2.2849\n",
      "Epoch: 2/20... Step: 260... Loss: 2.2648... Val Loss: 2.2587\n",
      "Epoch: 2/20... Step: 270... Loss: 2.2677... Val Loss: 2.2335\n",
      "Epoch: 3/20... Step: 280... Loss: 2.2587... Val Loss: 2.2103\n",
      "Epoch: 3/20... Step: 290... Loss: 2.2183... Val Loss: 2.1876\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1998... Val Loss: 2.1635\n",
      "Epoch: 3/20... Step: 310... Loss: 2.1667... Val Loss: 2.1413\n",
      "Epoch: 3/20... Step: 320... Loss: 2.1353... Val Loss: 2.1201\n",
      "Epoch: 3/20... Step: 330... Loss: 2.1032... Val Loss: 2.1029\n",
      "Epoch: 3/20... Step: 340... Loss: 2.1167... Val Loss: 2.0848\n",
      "Epoch: 3/20... Step: 350... Loss: 2.1153... Val Loss: 2.0654\n",
      "Epoch: 3/20... Step: 360... Loss: 2.0413... Val Loss: 2.0480\n",
      "Epoch: 3/20... Step: 370... Loss: 2.0652... Val Loss: 2.0272\n",
      "Epoch: 3/20... Step: 380... Loss: 2.0443... Val Loss: 2.0133\n",
      "Epoch: 3/20... Step: 390... Loss: 2.0161... Val Loss: 2.0000\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9858... Val Loss: 1.9793\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9929... Val Loss: 1.9603\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9857... Val Loss: 1.9498\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9715... Val Loss: 1.9327\n",
      "Epoch: 4/20... Step: 440... Loss: 1.9528... Val Loss: 1.9198\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8903... Val Loss: 1.9051\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8852... Val Loss: 1.8909\n",
      "Epoch: 4/20... Step: 470... Loss: 1.9090... Val Loss: 1.8811\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8980... Val Loss: 1.8699\n",
      "Epoch: 4/20... Step: 490... Loss: 1.9013... Val Loss: 1.8587\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8884... Val Loss: 1.8439\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8660... Val Loss: 1.8329\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8814... Val Loss: 1.8228\n",
      "Epoch: 4/20... Step: 530... Loss: 1.8399... Val Loss: 1.8128\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7977... Val Loss: 1.8025\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8384... Val Loss: 1.7902\n",
      "Epoch: 5/20... Step: 560... Loss: 1.8127... Val Loss: 1.7816\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7850... Val Loss: 1.7717\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7738... Val Loss: 1.7613\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7744... Val Loss: 1.7506\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7631... Val Loss: 1.7439\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7513... Val Loss: 1.7376\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7413... Val Loss: 1.7290\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7685... Val Loss: 1.7190\n",
      "Epoch: 5/20... Step: 640... Loss: 1.7354... Val Loss: 1.7117\n",
      "Epoch: 5/20... Step: 650... Loss: 1.7299... Val Loss: 1.7054\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6922... Val Loss: 1.6974\n",
      "Epoch: 5/20... Step: 670... Loss: 1.7245... Val Loss: 1.6884\n",
      "Epoch: 5/20... Step: 680... Loss: 1.7140... Val Loss: 1.6830\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6974... Val Loss: 1.6766\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6850... Val Loss: 1.6687\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6864... Val Loss: 1.6643\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6762... Val Loss: 1.6559\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6885... Val Loss: 1.6508\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6464... Val Loss: 1.6440\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6319... Val Loss: 1.6386\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6702... Val Loss: 1.6354\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6437... Val Loss: 1.6304\n",
      "Epoch: 6/20... Step: 780... Loss: 1.6342... Val Loss: 1.6222\n",
      "Epoch: 6/20... Step: 790... Loss: 1.6196... Val Loss: 1.6187\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6271... Val Loss: 1.6128\n",
      "Epoch: 6/20... Step: 810... Loss: 1.6109... Val Loss: 1.6084\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5876... Val Loss: 1.6060\n",
      "Epoch: 6/20... Step: 830... Loss: 1.6327... Val Loss: 1.5964\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5800... Val Loss: 1.5913\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5918... Val Loss: 1.5905\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5844... Val Loss: 1.5846\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5947... Val Loss: 1.5790\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5859... Val Loss: 1.5766\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5899... Val Loss: 1.5729\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5744... Val Loss: 1.5679\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5401... Val Loss: 1.5606\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5758... Val Loss: 1.5592\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5459... Val Loss: 1.5560\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5549... Val Loss: 1.5531\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5606... Val Loss: 1.5480\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5654... Val Loss: 1.5421\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5631... Val Loss: 1.5394\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5451... Val Loss: 1.5359\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5431... Val Loss: 1.5318\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5288... Val Loss: 1.5267\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5686... Val Loss: 1.5237\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5383... Val Loss: 1.5200\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.5250... Val Loss: 1.5192\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5196... Val Loss: 1.5190\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.5029... Val Loss: 1.5113\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.5046... Val Loss: 1.5076\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.5159... Val Loss: 1.5075\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.5157... Val Loss: 1.5041\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4917... Val Loss: 1.5011\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4871... Val Loss: 1.4959\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4989... Val Loss: 1.4938\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.5111... Val Loss: 1.4931\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4940... Val Loss: 1.4873\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4956... Val Loss: 1.4853\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.5119... Val Loss: 1.4845\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4670... Val Loss: 1.4796\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4795... Val Loss: 1.4787\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4746... Val Loss: 1.4798\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.5052... Val Loss: 1.4756\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4525... Val Loss: 1.4733\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4662... Val Loss: 1.4698\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4640... Val Loss: 1.4668\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4495... Val Loss: 1.4663\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4517... Val Loss: 1.4603\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4585... Val Loss: 1.4591\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4575... Val Loss: 1.4608\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4527... Val Loss: 1.4527\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4627... Val Loss: 1.4509\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4542... Val Loss: 1.4503\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4386... Val Loss: 1.4466\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4651... Val Loss: 1.4466\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4254... Val Loss: 1.4453\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4306... Val Loss: 1.4452\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.4132... Val Loss: 1.4398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.4076... Val Loss: 1.4387\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.4167... Val Loss: 1.4364\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3937... Val Loss: 1.4366\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4342... Val Loss: 1.4304\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4441... Val Loss: 1.4278\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4534... Val Loss: 1.4299\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4605... Val Loss: 1.4258\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4489... Val Loss: 1.4208\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4122... Val Loss: 1.4230\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4362... Val Loss: 1.4172\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3694... Val Loss: 1.4203\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3932... Val Loss: 1.4161\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3848... Val Loss: 1.4191\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.4043... Val Loss: 1.4135\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.4001... Val Loss: 1.4142\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3797... Val Loss: 1.4138\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3718... Val Loss: 1.4109\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.4026... Val Loss: 1.4050\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4506... Val Loss: 1.4050\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.4079... Val Loss: 1.4050\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.4092... Val Loss: 1.4044\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4167... Val Loss: 1.4010\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3688... Val Loss: 1.4010\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3512... Val Loss: 1.3982\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3474... Val Loss: 1.3980\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3729... Val Loss: 1.3997\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3593... Val Loss: 1.4020\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3647... Val Loss: 1.3953\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3850... Val Loss: 1.3973\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3582... Val Loss: 1.3948\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3288... Val Loss: 1.3908\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3959... Val Loss: 1.3881\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3691... Val Loss: 1.3873\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3735... Val Loss: 1.3857\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3474... Val Loss: 1.3841\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3546... Val Loss: 1.3825\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3370... Val Loss: 1.3811\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3418... Val Loss: 1.3788\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3791... Val Loss: 1.3778\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3405... Val Loss: 1.3782\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3124... Val Loss: 1.3841\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3428... Val Loss: 1.3779\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3634... Val Loss: 1.3779\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3422... Val Loss: 1.3743\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3245... Val Loss: 1.3763\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3478... Val Loss: 1.3714\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3633... Val Loss: 1.3706\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3363... Val Loss: 1.3693\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3555... Val Loss: 1.3679\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2993... Val Loss: 1.3687\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2803... Val Loss: 1.3651\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3486... Val Loss: 1.3656\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3445... Val Loss: 1.3635\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3350... Val Loss: 1.3643\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3651... Val Loss: 1.3634\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3404... Val Loss: 1.3619\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3288... Val Loss: 1.3637\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3286... Val Loss: 1.3608\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2934... Val Loss: 1.3597\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3406... Val Loss: 1.3574\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3151... Val Loss: 1.3602\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3155... Val Loss: 1.3534\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3139... Val Loss: 1.3534\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.3006... Val Loss: 1.3549\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.3078... Val Loss: 1.3502\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2922... Val Loss: 1.3493\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.3206... Val Loss: 1.3467\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3287... Val Loss: 1.3484\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2944... Val Loss: 1.3498\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.3075... Val Loss: 1.3475\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.3023... Val Loss: 1.3481\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.3086... Val Loss: 1.3481\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3192... Val Loss: 1.3461\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.3044... Val Loss: 1.3431\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.3154... Val Loss: 1.3437\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2973... Val Loss: 1.3453\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2892... Val Loss: 1.3405\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.3042... Val Loss: 1.3413\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2770... Val Loss: 1.3410\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2953... Val Loss: 1.3383\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.3189... Val Loss: 1.3368\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2880... Val Loss: 1.3381\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2886... Val Loss: 1.3363\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2768... Val Loss: 1.3365\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.3110... Val Loss: 1.3374\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2825... Val Loss: 1.3350\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2445... Val Loss: 1.3331\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2968... Val Loss: 1.3325\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2741... Val Loss: 1.3291\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2791... Val Loss: 1.3294\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2661... Val Loss: 1.3322\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2732... Val Loss: 1.3280\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2845... Val Loss: 1.3284\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2783... Val Loss: 1.3248\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2729... Val Loss: 1.3255\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2462... Val Loss: 1.3256\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2758... Val Loss: 1.3255\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2664... Val Loss: 1.3236\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2693... Val Loss: 1.3240\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2852... Val Loss: 1.3251\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2840... Val Loss: 1.3213\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2901... Val Loss: 1.3194\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2574... Val Loss: 1.3210\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2642... Val Loss: 1.3209\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2601... Val Loss: 1.3185\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2873... Val Loss: 1.3172\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2827... Val Loss: 1.3194\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2690... Val Loss: 1.3158\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2744... Val Loss: 1.3160\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2643... Val Loss: 1.3212\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2439... Val Loss: 1.3167\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2649... Val Loss: 1.3171\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2661... Val Loss: 1.3145\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2532... Val Loss: 1.3154\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2495... Val Loss: 1.3162\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2526... Val Loss: 1.3139\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2621... Val Loss: 1.3171\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2725... Val Loss: 1.3111\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2780... Val Loss: 1.3110\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2826... Val Loss: 1.3102\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2477... Val Loss: 1.3153\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2631... Val Loss: 1.3103\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2441... Val Loss: 1.3108\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2735... Val Loss: 1.3120\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2373... Val Loss: 1.3096\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2386... Val Loss: 1.3099\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2466... Val Loss: 1.3089\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2427... Val Loss: 1.3095\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2398... Val Loss: 1.3077\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2580... Val Loss: 1.3059\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2488... Val Loss: 1.3123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.2602... Val Loss: 1.3032\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2596... Val Loss: 1.3002\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2579... Val Loss: 1.2985\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2449... Val Loss: 1.3035\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2532... Val Loss: 1.2979\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2266... Val Loss: 1.2982\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2219... Val Loss: 1.3026\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2182... Val Loss: 1.3017\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.2190... Val Loss: 1.3026\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2259... Val Loss: 1.3020\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2165... Val Loss: 1.3010\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2574... Val Loss: 1.2999\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2761... Val Loss: 1.2996\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size= batch_size, \\\n",
    "      seq_length= seq_length, lr=0.001, print_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),\"char_level_lstm.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
